[
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "For more details and further functionality, please refer to the usage documentation and the parameter documentation.\nTo see the results of an example test run with a full size dataset refer to the results tab on the nf-core website pipeline page. For more details about the output files and reports, please refer to the output documentation.\n\n\nThe primary input to the pipeline is a TSV (tab-separated value) or CSV (comma comma-separated value) file, specified using the --sample_data option. This can be made in a spreadsheet program like LibreOffice Calc or Microsoft Excel by exporting to TSV. Columns can be in any order and unneeded columns can be left out or left blank. Column names are case insensitive and spaces are equivalent to underscores and can be left out. Only a single column containing either paths to raw sequence data, SRA (Sequence Read Archive) accessions, or NCBI queries to search the SRA is required and each sample can have values in different columns. Any columns not recognized by pathogensurveillance will be ignored, allowing users to adapt existing sample metadata table by adding new columns. Below is a description of each column used by pathogensurveillance:\n\nsample_id: The unique identifier for each sample. This will be used in file names to distinguish samples in the output. Each sample ID must correspond to a single source of sequence data (e.g. the path and ncbi_accession columns), although the same sequence data can be used by different IDs. Any values supplied that correspond to different sources of sequence data or contain characters that cannot appear in file names (/:*?“&lt;&gt;| .) will be modified automatically. If not supplied, it will be inferred from the path, ncbi_accession, or name columns.\nname: A human-readable label for the sample that is used in plots and tables. If not supplied, it will be inferred from sample_id.\ndescription: A longer human-readable label that is used in plots and tables. If not supplied, it will be inferred from name.\npath: Path to input sequence data, typically gzipped FASTQ files. When paired end sequencing is used, this is used for the forward read’s data and path_2 is used for the reverse reads. This can be a local file path or a URL to an online location. The sequence_type column must have a value.\npath_2: Path to the FASTQ files for the reverse read when paired-end sequencing is used. This can be a local file path or a URL to an online location. The sequence_type column must have a value.\nncbi_accession: An SRA accession ID for reads to be downloaded and used as samples. Values in the sequence_type column will be looked up if not supplied.\nncbi_query: A valid NCBI search query to search the SRA for reads to download and use as samples. This will result in an unknown number of samples being analyzed. The total number downloaded is limited by the ncbi_query_max column. Values in the sample_id, name, and description columns will be append to that supplied by the user. Values in the sequence_type column will be looked up and does not need to be supplied by the user.\nncbi_query_max: The maximum number or percentage of samples downloaded for the corresponding query in the ncbi_query column. Adding a % to the end of a number indicates a percentage of the total number of results instead of a count. A random of subset of results will be downloaded if ncbi_query_max is less than “100%” or the total number of results.\nsequence_type: The type of sequencing used to produce reads for the reads_1 and reads_2 columns. Valid values include anything containing the words “illumina”, “nanopore”, or “pacbio”. Will be looked up automatically for ncbi_accession and ncbi_query inputs but must be supplied by the user for path inputs.\nreport_group_ids: How to group samples into reports. For every unique value in this column a report will be generated. Samples can be assigned to multiple reports by separating group IDs by “;”. For example all;subset will put the sample in both all and subset report groups. Samples will be added to a default group if this is not supplied.\ncolor_by: The names of other columns that contain values used to color samples in plots and figures in the report. Multiple column names can be separated by “;”. Specified columns can contain either categorical factors or specific colors, specified as a hex code. By default, samples will be one color and references another.\nploidy: The ploidy of the sample. Should be a number. Defaults to “1”.\nenabled: Either “TRUE” or “FALSE”, indicating whether the sample should be included in the analysis or not. Defaults to “TRUE”.\nref_group_ids: One or more reference group IDs separated by “;”. These are used to supply specific references to specific samples. These IDs correspond to IDs listed in the ref_group_ids or ref_id columns of the reference metadata TSV.\n\nAdditionally, users can supply a reference metadata TSV/CSV that can be used to assign custom references to particular samples using the --reference_data option. If not provided, the pipeline will download and choose references to use automatically. References are assigned to samples if they share a reference group ID in the ref_group_ids columns that can appear in both input TSVs/CSVs. The reference metadata TSV or the sample metadata TSV can have the following columns:\n\nref_group_ids: One or more reference group IDs separated by “;”. These are used to group references and supply an ID that can be used in the ref_group_ids column of the sample metadata TSV/CSV to assign references to particular samples.\nref_id: The unique identifier for each user-defined reference genome. This will be used in file names to distinguish samples in the output. Each reference ID must correspond to a single source of reference data (The ref_path, ref_ncbi_accession, and ref_ncbi_query columns), although the same reference data can be used by multiple IDs. Any values that correspond to different sources of reference data or contain characters that cannot appear in file names (/:*?“&lt;&gt;| .) will be modified automatically. If not supplied, it will be inferred from the path, ref_name columns or supplied automatically when ref_ncbi_accession or ref_ncbi_query are used.\nref_id: The unique identify for each reference input. This will be used in file names to distinguish references in the output. Each sample ID must correspond to a single source of reference data (e.g. the ref_path and ref_ncbi_accession columns), although the same sequence data can be used by different IDs. Any values supplied that correspond to different sources of reference data or contain characters that cannot appear in file names (/:*?“&lt;&gt;| .) will be modified automatically. If not supplied, it will be inferred from the ref_path, ref_ncbi_accession, or ref_name columns.\nref_name: A human-readable label for user-defined reference genomes that is used in plots and tables. If not supplied, it will be inferred from ref_id. It will be supplied automatically when the ref_ncbi_query column is used.\nref_description: A longer human-readable label for user-defined reference genomes that is used in plots and tables. If not supplied, it will be inferred from ref_name. It will be supplied automatically when the ref_ncbi_query column is used.\nref_path: Path to user-defined reference genomes for each sample. This can be a local file path or a URL to an online location.\nref_ncbi_accession: RefSeq accession ID for a user-defined reference genome. These will be automatically downloaded and used as input.\nref_ncbi_query: A valid NCBI search query to search the assembly database for genomes to download and use as references. This will result in an unknown number of references being downloaded. The total number downloaded is limited by the ref_ncbi_query_max column. Values in the ref_id, ref_name, and ref_description columns will be append to that supplied by the user.\nref_ncbi_query_max: The maximum number or percentage of references downloaded for the corresponding query in the ref_ncbi_query column. Adding a % to the end of a number indicates a percentage of the total number of results instead of a count. A random of subset of results will be downloaded if ncbi_query_max is less than “100%” or the total number of results.\nref_primary_usage: Controls how the reference is used in the analysis in cases where a single “best” reference is required, such as for variant calling. Can be one of “optional” (can be used if selected by the analysis), “required” (will always be used), “exclusive” (only those marked “exclusive” will be used), or “excluded” (will not be used).\nref_contextual_usage: Controls how the reference is used in the analysis in cases where multiple references are required to provide context for the samples, such as for phylogeny. Can be one of “optional” (can be used if selected by the analysis), “required” (will always be used), “exclusive” (only those marked “exclusive” will be used), or “excluded” (will not be used).\nref_color_by: The names of other columns that contain values used to color references in plots and figures in the report. Multiple column names can be separated by “;”. Specified columns can contain either categorical factors or specific colors, specified as a hex code. By default, samples will be one color and references another.\nref_enabled: Either “TRUE” or “FALSE”, indicating whether the reference should be included in the analysis or not. Defaults to “TRUE”.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation.html#benchmarks",
    "href": "documentation.html#benchmarks",
    "title": "Documentation",
    "section": "Benchmarks",
    "text": "Benchmarks\nxanthomonas.csv\n(with all reads + Bakta database already downloaded):\n\n29 samples\nRun duration: 2h 16m 7s\nStorage:\n\nlocal reads folder: 11 GB\ncache: 75 GB\noutput directory: 19 MB\n\nhardware specs:\n\nOS: Pop!_OS 22.04LTS\nProcessor: 5.7 GHz Ryzen 9 7950X (16 cores - 32 threads)\nRAM: 128 GB DDR5 3600MHz (4x32)",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "NOTE: THIS PROJECT IS UNDER DEVELOPMENT AND MAY NOT FUNCTION AS EXPECTED UNTIL THIS MESSAGE GOES AWAY",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nnf-core/pathogensurveillance is a population genomic pipeline for pathogen diagnosis, variant detection, and biosurveillance. The pipeline accepts the paths to raw reads for one or more organisms (in the form of a CSV file) and creates reports in the form of interactive HTML reports or PDF documents. Significant features include the ability to analyze unidentified eukaryotic and prokaryotic samples, creation of reports for multiple user-defined groupings of samples, automated discovery and downloading of reference assemblies from NCBI RefSeq, and rapid initial identification based on k-mer sketches followed by a more robust core genome phylogeny and SNP-based phylogeny.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#pipelinesummary",
    "href": "index.html#pipelinesummary",
    "title": "",
    "section": "Pipeline summary",
    "text": "Pipeline summary\n\nThis is a quick breakdown of the processes used by pathogensurveillance:\n\nDownload sequences and references if they are not provided locally\nGenome assembly\n\nIllumina shortreads: (spades)\nPacbio or Oxford Nanopore longreads: (flye)\n\nQuickly obtain several initial sample references (bbmap)\nMore accurately select appropriate reference genomes. Genome “sketches” are compared between first-pass references, samples, and any references directly provided by the user (sourmash)\nGenome annotation (bakta)\nAlign reads to reference sequences (bwa)\nVariant calling and filtering (graphtyper, vcflib)\nDetermine relationship between samples and references\n\nBuild SNP tree from variant calls (iqtree)\nFor Prokaryotes:\n\nIdentify shared orthologs (pirate)\nBuild tree from core genome phylogeny (iqtree)\n\nFor Eukaryotes:\n\nIdentify BUSCO genes (busco)\nBuild tree from BUSCO genes (iqtree)\n\n\nGenerate interactive html report/pdf file\n\nSequence and assembly information (fastqc, multiqc, quast)\nsample identification tables and heatmaps\nPhylogenetic trees from genome-wide SNPs and core genes\nminimum spanning network",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#flowchart",
    "href": "index.html#flowchart",
    "title": "",
    "section": "PathogenSurveillance pipeline chart",
    "text": "PathogenSurveillance pipeline chart",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "Install Nextflow (&gt;=21.10.3)\nInstall any of Docker, Singularity (you can follow this tutorial), Podman, Shifter or Charliecloud for full pipeline reproducibility (you can use Conda both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see docs).\nDownload the pipeline and test it on a minimal dataset with a single command:\nnextflow run nf-core/pathogensurveillance -profile test,YOURPROFILE --outdir &lt;OUTDIR&gt; --download_bakta_db true -resume \nNote that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (YOURPROFILE in the example command above). You can chain multiple config profiles in a comma-separated string.\n\n\nThe pipeline comes with config profiles called docker, singularity, podman, shifter, charliecloud and conda which instruct the pipeline to use the named tool for software management. For example, -profile test,docker.\nPlease check nf-core/configs to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use -profile &lt;institute&gt; in your command. This will enable either docker or singularity and set the appropriate execution settings for your local compute environment.\nIf you are using singularity, please use the nf-core download command to download images first, before running the pipeline. Setting the NXF_SINGULARITY_CACHEDIR or singularity.cacheDir Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\nIf you are using conda, it is highly recommended to use the NXF_CONDA_CACHEDIR or conda.cacheDir settings to store the environments in a central location for future pipeline runs.\n\n\nStart running your own analysis:\n\nnextflow run nf-core/pathogensurveillance --input samplesheet.csv --outdir &lt;OUTDIR&gt; --download_bakta_db true -profile &lt;docker/singularity/podman/shifter/charliecloud/conda/institute&gt; -resume\n\nYou can also try running a small example dataset hosted with the source code using the following command (no need to download anything):\nnextflow run nf-core/pathogensurveillance --input https://raw.githubusercontent.com/grunwaldlab/pathogensurveillance/master/test/data/metadata_small.csv --outdir test_out --download_bakta_db true -profile docker -resume"
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Tutorial",
    "section": "",
    "text": "Before starting, first take a look at the Quickstart for instructions on how to download pathogensurveillance and install both Docker and Nextflow.",
    "crumbs": [
      "Tutorial"
    ]
  },
  {
    "objectID": "citations.html",
    "href": "citations.html",
    "title": "Citations",
    "section": "",
    "text": "nf-core/pathogensurveillance was written by:\nZachary S. L. Foster1, Martha Sudermann2, Camilo Parada-Rojas2, Fernanda Iruegas-Bocardo2, Ricardo Alcalá-Briseño2, Logan K. Blair 1, Alexandra J Weisberg2, Jeff H. Chang2, and Niklaus J. Grünwald1\n1Horticultural Crops Research Laboratory, USDA Agricultural Research Service, Corvallis, Oregon 97331, USA\n2Department of Botany and Plant Pathology, Oregon State University, Corvallis, Oregon 97331, USA",
    "crumbs": [
      "Citations"
    ]
  },
  {
    "objectID": "citations.html#credits",
    "href": "citations.html#credits",
    "title": "Citations",
    "section": "",
    "text": "nf-core/pathogensurveillance was written by:\nZachary S. L. Foster1, Martha Sudermann2, Camilo Parada-Rojas2, Fernanda Iruegas-Bocardo2, Ricardo Alcalá-Briseño2, Logan K. Blair 1, Alexandra J Weisberg2, Jeff H. Chang2, and Niklaus J. Grünwald1\n1Horticultural Crops Research Laboratory, USDA Agricultural Research Service, Corvallis, Oregon 97331, USA\n2Department of Botany and Plant Pathology, Oregon State University, Corvallis, Oregon 97331, USA",
    "crumbs": [
      "Citations"
    ]
  },
  {
    "objectID": "citations.html#contributionsandsupport",
    "href": "citations.html#contributionsandsupport",
    "title": "Citations",
    "section": "Contributions and Support",
    "text": "Contributions and Support\nIf you would like to contribute to this pipeline, please see the contributing guidelines.\nFor further information or help, don’t hesitate to get in touch on the Slack #pathogensurveillance channel (you can join with this invite).",
    "crumbs": [
      "Citations"
    ]
  },
  {
    "objectID": "citations.html#citations",
    "href": "citations.html#citations",
    "title": "Citations",
    "section": "Citations",
    "text": "Citations\n\n\n\nNote: these citations are for all possible programs that may be used by pathogensurveillance. The final report will only present citations of programs used in the current run.\n\n\nAndrews, Simon et al. 2010. “FastQC: A Quality Control Tool for High Throughput Sequence Data.” Cambridge, United Kingdom.\n\n\nBayliss, Sion C, Harry A Thorpe, Nicola M Coyle, Samuel K Sheppard, and Edward J Feil. 2019. “PIRATE: A Fast and Scalable Pangenomics Toolbox for Clustering Diverged Orthologues in Bacteria.” Gigascience 8 (10): giz119.\n\n\nBrown, C Titus, and Luiz Irber. 2016. “Sourmash: A Library for MinHash Sketching of DNA.” Journal of Open Source Software 1 (5): 27.\n\n\nBushnell, Brian. 2014. “BBMap: A Fast, Accurate, Splice-Aware Aligner.”\n\n\nChen, Shifu. 2023. “Ultrafast One-Pass FASTQ Data Preprocessing, Quality Control, and Deduplication Using Fastp.” Imeta 2 (2): e107.\n\n\nCrusoe, Michael R, Hussien F Alameldin, Sherine Awad, Elmar Boucher, Adam Caldwell, Reed Cartwright, Amanda Charbonneau, et al. 2015. “The Khmer Software Package: Enabling Efficient Nucleotide Sequence Analysis.” F1000Research 4.\n\n\nDanecek, Petr, Adam Auton, Goncalo Abecasis, Cornelis A Albers, Eric Banks, Mark A DePristo, Robert E Handsaker, et al. 2011. “The Variant Call Format and VCFtools.” Bioinformatics 27 (15): 2156–58.\n\n\nDanecek, Petr, James K Bonfield, Jennifer Liddle, John Marshall, Valeriu Ohan, Martin O Pollard, Andrew Whitwham, et al. 2021. “Twelve Years of SAMtools and BCFtools.” Gigascience 10 (2): giab008.\n\n\nDi Tommaso, Paolo, Maria Chatzou, Evan W Floden, Pablo Prieto Barja, Emilio Palumbo, and Cedric Notredame. 2017. “Nextflow Enables Reproducible Computational Workflows.” Nature Biotechnology 35 (4): 316–19.\n\n\nDistribution, Anaconda Software. 2016. “Computer Software.” Vers. 4: 2–2.\n\n\nDylus, David, Adrian Altenhoff, Sina Majidian, Fritz J Sedlazeck, and Christophe Dessimoz. 2024. “Inference of Phylogenetic Trees Directly from Raw Sequencing Reads Using Read2Tree.” Nature Biotechnology 42 (1): 139–47.\n\n\nEggertsson, Hannes P, Hakon Jonsson, Snaedis Kristmundsdottir, Eirikur Hjartarson, Birte Kehr, Gisli Masson, Florian Zink, et al. 2017. “Graphtyper Enables Population-Scale Genotyping Using Pangenome Graphs.” Nature Genetics 49 (11): 1654–60.\n\n\nEwels, Philip A, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso, and Sven Nahnsen. 2020. “The Nf-Core Framework for Community-Curated Bioinformatics Pipelines.” Nature Biotechnology 38 (3): 276–78.\n\n\nEwels, Philip, Måns Magnusson, Sverker Lundin, and Max Käller. 2016. “MultiQC: Summarize Analysis Results for Multiple Tools and Samples in a Single Report.” Bioinformatics 32 (19): 3047–48.\n\n\nGarrison, Erik, Zev N Kronenberg, Eric T Dawson, Brent S Pedersen, and Pjotr Prins. 2022. “A Spectrum of Free Software Tools for Processing the VCF Variant Call Format: Vcflib, Bio-Vcf, Cyvcf2, Hts-Nim and Slivar.” PLoS Computational Biology 18 (5): e1009123.\n\n\nGrüning, Björn, Ryan Dale, Andreas Sjödin, Brad A Chapman, Jillian Rowe, Christopher H Tomkins-Tinch, Renan Valieris, Johannes Köster, and Bioconda Team. 2018. “Bioconda: Sustainable and Comprehensive Software Distribution for the Life Sciences.” Nature Methods 15 (7): 475–76.\n\n\nKamvar, Zhian N, Javier F Tabima, and Niklaus J Grünwald. 2014. “Poppr: An r Package for Genetic Analysis of Populations with Clonal, Partially Clonal, and/or Sexual Reproduction.” PeerJ 2: e281.\n\n\nKatoh, Kazutaka, Kazuharu Misawa, Kei-ichi Kuma, and Takashi Miyata. 2002. “MAFFT: A Novel Method for Rapid Multiple Sequence Alignment Based on Fast Fourier Transform.” Nucleic Acids Research 30 (14): 3059–66.\n\n\nKolmogorov, Mikhail, Jeffrey Yuan, Yu Lin, and Pavel A Pevzner. 2019. “Assembly of Long, Error-Prone Reads Using Repeat Graphs.” Nature Biotechnology 37 (5): 540–46.\n\n\nKurtzer, Gregory M, Vanessa Sochat, and Michael W Bauer. 2017. “Singularity: Scientific Containers for Mobility of Compute.” PloS One 12 (5): e0177459.\n\n\nLi, Heng. 2011. “Tabix: Fast Retrieval of Sequence Features from Generic TAB-Delimited Files.” Bioinformatics 27 (5): 718–19.\n\n\nLi, Heng, and Richard Durbin. 2009. “Fast and Accurate Short Read Alignment with Burrows–Wheeler Transform.” Bioinformatics 25 (14): 1754–60.\n\n\nManni, Mosè, Matthew R Berkeley, Mathieu Seppey, Felipe A Simão, and Evgeny M Zdobnov. 2021. “BUSCO Update: Novel and Streamlined Workflows Along with Broader and Deeper Phylogenetic Coverage for Scoring of Eukaryotic, Prokaryotic, and Viral Genomes.” Molecular Biology and Evolution 38 (10): 4647–54.\n\n\nMikheenko, Alla, Andrey Prjibelski, Vladislav Saveliev, Dmitry Antipov, and Alexey Gurevich. 2018. “Versatile Genome Assembly Evaluation with QUAST-LG.” Bioinformatics 34 (13): i142–50.\n\n\nNguyen, Lam-Tung, Heiko A Schmidt, Arndt Von Haeseler, and Bui Quang Minh. 2015. “IQ-TREE: A Fast and Effective Stochastic Algorithm for Estimating Maximum-Likelihood Phylogenies.” Molecular Biology and Evolution 32 (1): 268–74.\n\n\n“Picard Toolkit.” 2019. Broad Institute, GitHub Repository. https://broadinstitute.github.io/picard/; Broad Institute.\n\n\nPrjibelski, Andrey, Dmitry Antipov, Dmitry Meleshko, Alla Lapidus, and Anton Korobeynikov. 2020. “Using SPAdes de Novo Assembler.” Current Protocols in Bioinformatics 70 (1): e102.\n\n\nQin, Qi-Long, Bin-Bin Xie, Xi-Ying Zhang, Xiu-Lan Chen, Bai-Cheng Zhou, Jizhong Zhou, Aharon Oren, and Yu-Zhong Zhang. 2014. “A Proposed Genus Boundary for the Prokaryotes Based on Genomic Insights.” Journal of Bacteriology 196 (12): 2210–15.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSayers, Eric W, Evan E Bolton, J Rodney Brister, Kathi Canese, Jessica Chan, Donald C Comeau, Ryan Connor, et al. 2022. “Database Resources of the National Center for Biotechnology Information.” Nucleic Acids Research 50 (D1): D20.\n\n\nSchwengers, Oliver, Lukas Jelonek, Marius Alfred Dieckmann, Sebastian Beyvers, Jochen Blom, and Alexander Goesmann. 2021. “Bakta: Rapid and Standardized Annotation of Bacterial Genomes via Alignment-Free Sequence Identification.” Microbial Genomics 7 (11): 000685.\n\n\nShen, Wei, Shuai Le, Yan Li, and Fuquan Hu. 2016. “SeqKit: A Cross-Platform and Ultrafast Toolkit for FASTA/q File Manipulation.” PloS One 11 (10): e0163962.\n\n\nVan der Auwera, Geraldine A, and Brian D O’Connor. 2020. Genomics in the Cloud: Using Docker, GATK, and WDL in Terra. O’Reilly Media.\n\n\nVeiga Leprevost, Felipe da, Björn A Grüning, Saulo Alves Aflitos, Hannes L Röst, Julian Uszkoreit, Harald Barsnes, Marc Vaudel, et al. 2017. “BioContainers: An Open-Source and Community-Driven Framework for Software Standardization.” Bioinformatics 33 (16): 2580–82.",
    "crumbs": [
      "Citations"
    ]
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "faq",
    "section": "",
    "text": "How long does the pathogensurveillance take to run?\nHow many samples can be run at once?\nWhat are the minimum computational resources necessary to run pathogensurveillance?\nHow much sequencing coverage is needed?\nCan pathogensurveillance be run on macOS?\nHow are reference genomes selected?\nThis process works remarkably well for most use cases. However, there are some biological reasons why selecting a reference genome becomes a more challenging problem (with or without pathogensurveilance). First, if you are working with a truly unique organism, there may be no nearby reference. Second, closely related bacteria can still vary considerably their accessory genomes. This process will prioritze matches between core genes above other genomic content, like extra-chromosomal plasmids or other rapidly evolving/repetitive regions. If these regions are of diagnostic interest for your system, you may get better results if you are able to specify a known reference before running the pipeline."
  },
  {
    "objectID": "examplereports.html",
    "href": "examplereports.html",
    "title": "Example Reports",
    "section": "",
    "text": "example 1 report\n\nIruegas-Bocardo, Fernanda, et al. “Whole genome sequencing-based tracing of a 2022 introduction and outbreak of Xanthomonas hortorum pv. pelargonii.” Phytopathology® 113.6 (2023): 975-984.",
    "crumbs": [
      "Example Reports"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "tutorial.html#snp-trees",
    "href": "tutorial.html#snp-trees",
    "title": "Tutorial",
    "section": "\nSNP trees\n",
    "text": "SNP trees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout this plot\n\n\n\n\n\n\n\n\nThis is a representation of a Single Nucleotide Polymorphism (SNP) tree, depicting the genetic relationships among samples in comparison to a reference assembly.\n\n\nThe tree is less robust than a core gene phylogeny and cannot offer insights on evolutionary relationships among strains, but it does offer one way to visualize the genetic diversity among samples, with genetically similar strains clustering together.\n\n\nQuestion-does it make sense to be showing the reference within the tree?",
    "crumbs": [
      "Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#minimum-spanning-network",
    "href": "tutorial.html#minimum-spanning-network",
    "title": "Tutorial",
    "section": "\nMinimum spanning network\n",
    "text": "Minimum spanning network\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout this plot\n\n\n\n\n\n\n\n\nThis figure depicts a minimium spanning network (MSN). The nodes represent unique multiocus genotypes, and the size of nodes is proportional to the # number of samples that share the same genotype.\n\n\nThe edges represent the SNP differences between two given genotypes, and the darker the color of the edges, the fewer SNP differences between the two.\n\n\nNote: within these MSNs, edge lengths are not proportional to SNP differences.",
    "crumbs": [
      "Tutorial"
    ]
  },
  {
    "objectID": "index.html#background---why-use-pathogensurveilance",
    "href": "index.html#background---why-use-pathogensurveilance",
    "title": "",
    "section": "Background - why use pathogensurveilance?",
    "text": "Background - why use pathogensurveilance?\nTL;DR:\nunknown FASTQ -&gt; sample ID + phylogeny + publication-quality figures\nMost existing genomic tools are designed to be used with a reference genome. Unfortunately, this is a luxury in the pathogen diagnostic world, where researchers often are faced with unknown samples. This creates a seemingly impossible problem: how can you draw a point of comparison without any starting point?\nPathogensurveilance addresses this by picking a good reference genome for you. In most cases, this will be the best reference. It is therefore a great option for identification of an unknown sample, or when you would like some empirical data suggesting which references are available in nearby species.\nPathogensurveilance uses several strategies to address the emerging complexity of working with sequences that could anything from plants to bacteria to fungus. First, pathogensurveilance uses reasonable baseline parameters that work very well with most species. There are some processes where a one-size-fits-all strategy isn’t feasible. In such cases, Pathogensurveilance will change the analysis workflow automatically (e.g. assembling Prokaryotic vs Eukaryotic core genes). These analysis branchpoints are facilitated by the Nextflow framework, and require no additional inputs from the user.\nWhile pathogensurveilance may be a useful tool for researchers of all levels, it was designed with clinicians in mind who may have limited bioinformatics training. Pathogensurveilance is very simple to run. At a minimum, all that needs to be supplied is a .CSV file with a single column specifying the path to your sample’s sequencing reads. For more experienced users, there are opportunities to customize the parameters of the pipeline. In particular, the final report is designed for configuration through by nature of Quarto and the psminer plugin system.\nPathogensurveilance is particularly good for:\n\nunknown sample ID\nexploratory population analysis using minimal input parameters\ninexperienced bioinformatics users\nrepeated analysis where you would like to compare a few new samples to a past run.\n\nPathogensurveilance is not designed for:\n\nviral sequence\nnon gDNA sequences (RNA-seq, RAD-seq, ChIP-seq, etc.)\nExperienced researchers who want to use a highly customizable pipeline"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "",
    "section": "Background - why use pathogensurveillance?",
    "text": "Background - why use pathogensurveillance?\nTL;DR: unknown gDNA FASTQ -&gt; sample ID + phylogeny + publication-quality figures\nMost genomic tools are designed to be used with a reference genome. Yet this is at odds with the work of pathogen diagnosticians, who often deal with unknown samples. Finding the right reference manually may be cumbersome and require a suprising amount of technical skill.\nPathogenSurveillance picks a good reference genome for you. It does this through the program sourmash. In simple terms, this takes a sample’s DNA “fingerprint” and finds the closest match in a “DNA fingerprint library” spanning the tree of life. In more technical terms, the pipeline generates k-mer sketches from your reads assembled into genomes, then uses the identified reference to do a boilerplate, but robust phylogenetic analysis of your submitted samples.\nIn our experience, the pipeline usually chooses the best possible reference genome. At a minimum it will choose a reference that is good enough to build an informative phylogeny and allow you to see the contextual placement your samples.\nPathogensurveillance is designed to use as many types of genomic DNA input as possible. It works for common shortread and longread sequencing technologies and for both prokaryotes and eukaryotes. There is a good deal of emergent complexity required to work with such a broad sample range, but pathogensurveillance handles this automatically.\nWhile PathogenSurveillance may be a useful tool for researchers of all levels, it was designed with those who may have limited bioinformatics training in mind. Pathogensurveillance is very simple to run. At a minimum, all that needs to be supplied is a .CSV file with a single column specifying the path to your sample’s sequencing reads. Other information is optional, but if provided will used to customize the output report or conditionally use particular reference genomes.\npathogensurveillance is particularly good for:\n\nunknown sample identification\nexploratory population analysis using minimal input parameters\ninexperienced bioinformatics users\nefficient parallelization of tasks\nrepeated analysis (given caching) where you would like to add new samples to a past run\n\n\n\n\n\n\n\nNote\n\n\n\npathogensurveillance works for non pathogens too!\n\n\npathogensurveillance is not designed for:\n\nviral sequence\nnon gDNA datasets (DNA assembly fasta files, RNA-seq, RAD-seq, ChIP-seq, etc.)\nmixed/impure samples (this may change in future versions)\nHighly specialized population genetic analysis, or researchers who would like to extensively test parameters at each stage",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "examplereports.html#xanthomonas",
    "href": "examplereports.html#xanthomonas",
    "title": "example reports",
    "section": "",
    "text": "Example 1\nData from Iruegas-Bocardo et al 2023:\n\nWhole Genome Sequencing-Based Tracing of a 2022 Introduction and Outbreak of Xanthomonas hortorum pv. pelargonii doi: 10.1094/PHYTO-09-22-0321-R."
  },
  {
    "objectID": "examplereports.html#mycobacterium",
    "href": "examplereports.html#mycobacterium",
    "title": "example reports",
    "section": "mycobacterium",
    "text": "mycobacterium\nExample 2\nData from Bryant et al. 2016:\n\nEmergence and spread of a human-transmissible multidrug-resistant nontuberculous mycobacterium doi: DOI: 10.1126/science.aaf8156."
  },
  {
    "objectID": "examplereports.html#example-1---xanthomonas",
    "href": "examplereports.html#example-1---xanthomonas",
    "title": "Example Reports",
    "section": "",
    "text": "example 1 report\n\nIruegas-Bocardo, Fernanda, et al. “Whole genome sequencing-based tracing of a 2022 introduction and outbreak of Xanthomonas hortorum pv. pelargonii.” Phytopathology® 113.6 (2023): 975-984.",
    "crumbs": [
      "Example Reports"
    ]
  },
  {
    "objectID": "examplereports.html#example-2---mycobacterium",
    "href": "examplereports.html#example-2---mycobacterium",
    "title": "Example Reports",
    "section": "Example 2 - mycobacterium",
    "text": "Example 2 - mycobacterium\nexample 2 report\n\nBryant, Josephine M., et al. “Population-level genomics identifies the emergence and global spread of a human transmissible multidrug-resistant nontuberculous mycobacterium.” Science (New York, NY) 354.6313 (2016): 751.",
    "crumbs": [
      "Example Reports"
    ]
  },
  {
    "objectID": "documentation.html#input-format",
    "href": "documentation.html#input-format",
    "title": "Documentation",
    "section": "Input format",
    "text": "Input format\n\nPrimary CSV file\nThe primary input to the pipeline is a CSV (comma comma-separated value). Columns can be in any order and unneeded columns can be left out or left blank. Only a single column containing paths to raw sequence data or SRA (Sequence Read Archive) accessions is required and each sample can have values in different columns. Any columns not recognized by pathogensurveillance will be ignored, allowing users to adapt existing sample metadata table by adding new columns. Below is a description of each column used by pathogensurveillance:\n\nsample_id: The unique identifier for each sample. This will be used in file names to distinguish samples in the output. Each sample ID must correspond to a single source of sequence data (e.g. the path and ncbi_accession columns), although the same sequence data can be used by different IDs. Any values supplied that correspond to different sources of sequence data or contain characters that cannot appear in file names (/:*?“&lt;&gt;| .) will be modified automatically. If not supplied, it will be inferred from the path, ncbi_accession, or name columns.\nname: A human-readable label for the sample that is used in plots and tables. If not supplied, it will be inferred from sample_id.\npath: Path to input sequence data, typically gzipped FASTQ files. When paired end sequencing is used, this is used for the forward read’s data and path_2 is used for the reverse reads. This can be a local file path or a URL to an online location. The sequence_type column must have a value.\npath_2: Path to the FASTQ files for the reverse read when paired-end sequencing is used. This can be a local file path or a URL to an online location. The sequence_type column must have a value.\nncbi_accession: An SRA accession ID for reads to be downloaded and used as samples. Values in the sequence_type column will be looked up if not supplied.\nncbi_query: A valid NCBI search query to search the SRA for reads to download and use as samples. This will result in an unknown number of samples being analyzed. The total number downloaded is limited by the ncbi_query_max column. Values in the sample_id, name, and description columns will be append to that supplied by the user. Values in the sequence_type column will be looked up and does not need to be supplied by the user.\nncbi_query_max: The maximum number or percentage of samples downloaded for the corresponding query in the ncbi_query column. Adding a % to the end of a number indicates a percentage of the total number of results instead of a count. A random of subset of results will be downloaded if ncbi_query_max is less than “100%” or the total number of results.\nsequence_type: The type of sequencing used to produce reads for the reads_1 and reads_2 columns. Valid values include anything containing the words “illumina”, “nanopore”, or “pacbio”. Will be looked up automatically for ncbi_accession and ncbi_query inputs but must be supplied by the user for path inputs.\nreport_group_ids: How to group samples into reports. For every unique value in this column a report will be generated. Samples can be assigned to multiple reports by separating group IDs by “;”. For example all;subset will put the sample in both all and subset report groups. Samples will be added to a default group if this is not supplied.\ncolor_by: The names of other columns that contain values used to color samples in plots and figures in the report. Multiple column names can be separated by “;”. Specified columns can contain either categorical factors or specific colors, specified as a hex code. By default, samples will be one color and references another.\nploidy: The ploidy of the sample. Should be a number. Defaults to “1”.\nenabled: Either “TRUE” or “FALSE”, indicating whether the sample should be included in the analysis or not. Defaults to “TRUE”.\nref_group_ids: One or more reference group IDs separated by “;”. These are used to supply specific references to specific samples. These IDs correspond to IDs listed in the ref_group_ids or ref_id columns of the reference metadata CSV.\n\n\n\nSpecifying custom references\nAdditionally, users can supply a reference metadata CSV that can be used to assign custom references to particular samples. This is an optional step References are assigned to samples if they share a reference group ID in the ref_group_ids columns that can appear in both input CSVs. The reference metadata CSV can have the following columns:\n\nref_group_ids: One or more reference group IDs separated by “;”. These are used to group references and supply an ID that can be used in the ref_group_ids column of the sample metadata CSV to assign references to particular samples.\nref_id: The unique identifier for each user-defined reference genome. This will be used in file names to distinguish samples in the output. Each reference ID must correspond to a single source of reference data (The ref_path, ref_ncbi_accession, and ref_ncbi_query columns), although the same reference data can be used by multiple IDs. Any values that correspond to different sources of reference data or contain characters that cannot appear in file names (/:*?“&lt;&gt;| .) will be modified automatically. If not supplied, it will be inferred from the path, ref_name columns or supplied automatically when ref_ncbi_accession or ref_ncbi_query are used.\nref_name: A human-readable label for user-defined reference genomes that is used in plots and tables. If not supplied, it will be inferred from ref_id. It will be supplied automatically when the ref_ncbi_query column is used.\nref_description: A longer human-readable label for user-defined reference genomes that is used in plots and tables. If not supplied, it will be inferred from ref_name. It will be supplied automatically when the ref_ncbi_query column is used.\nref_path: Path to user-defined reference genomes for each sample. This can be a local file path or a URL to an online location.\nref_ncbi_accession: RefSeq accession ID for a user-defined reference genome. These will be automatically downloaded and used as input.\nref_ncbi_query: A valid NCBI search query to search the assembly database for genomes to download and use as references. This will result in an unknown number of references being downloaded. The total number downloaded is limited by the ref_ncbi_query_max column. Values in the ref_id, ref_name, and ref_description columns will be append to that supplied by the user.\nref_ncbi_query_max: The maximum number or percentage of references downloaded for the corresponding query in the ref_ncbi_query column. Adding a % to the end of a number indicates a percentage of the total number of results instead of a count. A random of subset of results will be downloaded if ncbi_query_max is less than “100%” or the total number of results.\nref_primary_usage: Controls how the reference is used in the analysis in cases where a single “best” reference is required, such as for variant calling. Can be one of “optional” (can be used if selected by the analysis), “required” (will always be used), “exclusive” (only those marked “exclusive” will be used), or “excluded” (will not be used).\nref_contextual_usage: Controls how the reference is used in the analysis in cases where multiple references are required to provide context for the samples, such as for phylogeny. Can be one of “optional” (can be used if selected by the analysis), “required” (will always be used), “exclusive” (only those marked “exclusive” will be used), or “excluded” (will not be used).\nref_color_by: The names of other columns that contain values used to color references in plots and figures in the report. Multiple column names can be separated by “;”. Specified columns can contain either categorical factors or specific colors, specified as a hex code. By default, samples will be one color and references another.\nref_enabled: Either “TRUE” or “FALSE”, indicating whether the reference should be included in the analysis or not. Defaults to “TRUE”.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation.html#command-line-options",
    "href": "documentation.html#command-line-options",
    "title": "Documentation",
    "section": "",
    "text": "Required:\n\n--input: Path to comma-separated file containing information about the samples in the experiment.\n--output: The output directory where the results will be saved. You have to use absolute paths to storage on Cloud infrastructure.\n--bakta_db: The path to the Bakta database folder. This or --download_bakta_db must be included.\n--download_bakta_db: Download the database required for running Bakta. Note that this will download gigabytes of information, so if you plan on running the pipeline repeatedly it would be better to download the database manually and specify the path with --bakta_db.\n\nNextflow Options:\n\n-profile: Instructs the pipeline to use the named tool for software management. docker, singularity, podman, shifter, charliecloud and conda. For example, -profile test,docker\n-resume: Restarts an incomplete run by using cached intermediate files.\n\nOptional:\n\n--email: Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits. If set in your user config file (`~/.nextflow/config`) then you don’t need to specify this on the command line for every run.\n--multiqc_title: MultiQC report title. Printed as page header, used for filename if not otherwise specified.\n\nPerformance Parameters:\n\n--max_cpus: Maximum number of CPUs that can be requested for any single job. Default: 16\n--max_memory: Maximum amount of memory that can be requested for any single job. Default: 128.GB\n--max_time: Maximum amount of time that can be requested for any single job. Default: 240.h\n\nAnalysis Parameters:\n\n--sketch_max_depth: Depth reads are subsampled to for the initial sketch-based identification. Default: 3\n--variant_max_depth: Depth reads are subsampled to for the variant-based parts of the analysis. Default: 15\n--assembly_max_depth: Depth reads are subsampled to for genome assembly. This will be multiplied by the predicted ploidy of each sample. Default: 30\n--refseq_download_num: The maximum number of RefSeq sequences to select and download for each sample at each taxonomic level (species, genus, and family). The total number will vary based on the diversity of samples. Default: 10\n--min_core_genes: The minimum number of genes needed to conduct a core gene phylogeny. Samples and references will be removed (as allowed by the min_core_samps and min_core_refs options) until this minimum is met. Default: 10\n--min_core_samps: The minimum proportion of samples needed to conduct a core gene phylogeny. Samples will be removed until the min_core_genes option is satisfied or this minimum is met. Default: 0.8\n--min_core_refs: The minimum proportion of references needed to conduct a core gene phylogeny. References will be removed until the min_core_genes option is satisfied or this minimum is met. Default: 0.5\n--max_core_genes: The maximum number of genes used to conduct a core gene phylogeny. Default: 100\n--min_ref_ani: The minimum ANI between a sample and potential reference for that reference to be used for variant calling with that sample. To force all the samples in a report group to use the same reference, set this value very low. Default: 0.9\n--copymode: Storage management setting to determine which files will be copied from the cache into the output directory.\n\nhigh - All files are copied into output directory.\nmedium - Reports are copied. Large sequencing files are not copied, but they are accessible through symlinks to their location in the cache (default).\nlow - No files are copied into output directory, but files are accessible through symlinks to their location in the cache.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "tutorial.html#example-1-standard-run",
    "href": "tutorial.html#example-1-standard-run",
    "title": "Tutorial",
    "section": "Example 1: Standard Run",
    "text": "Example 1: Standard Run\nThis example uses sequencing reads from an 2022 outbreak of the bacterial pathogen Xanthomonas hortorum found infecting geranium in several plant nurseries. Using whole-genome sequencing, researchers determined a shared genetic basis between strains at different locations. With this information, they traced the origin of the outbreak to a single supplier that sold infected cuttings. You can read more about the study here. \nWe’ll be treating the pathogen as an unknown and using the pathogensurveillance pipeline to determine what we know already (that these samples come from Xanthomonas hortorum). We’ll also see the high degree of shared DNA sequence between samples, which is seen from several plots that the pathogensurveillance pipeline generates automatically. \n\nSample input\nThe pipeline is designed to work with a wide variety of existing metadata sheets without extensive changes. Here’s a look at “xanthomonas.csv”, which serves as the only unique input file within the command to run the pipeline:\n\n\nWarning: package 'tidyverse' was built under R version 4.1.2\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'forcats' was built under R version 4.1.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample_id\npath_1\npath_2\nsequence_type\nreference\nreference_id\nreport_group\ncolor_by\ndate_isolated\ndate_received\nyear\nhost\ncv_key\nnursery\nX\nX.1\n\n\n\n\n22-299\ntest/data/reads/22-299_R1.fastq.gz\ntest/data/reads/22-299_R2.fastq.gz\nIllumina\n\n\nxan_test\nsubgroup\nyear\nnursery\n3/2/22\n3/29/22\n2022\nPelargonium x hortorum\nCV-1\nMD\n\n\n22-300\ntest/data/reads/22-300_R1.fastq.gz\ntest/data/reads/22-300_R2.fastq.gz\nIllumina\n\n\nxan_test\nsubgroup\nyear\nnursery\n3/2/22\n3/30/22\n2022\nPelargonium x hortorum\nCV-2\nMD\n\n\n22-301\ntest/data/reads/22-301_R1.fastq.gz\ntest/data/reads/22-301_R2.fastq.gz\nIllumina\n\n\nxan_test\nsubgroup\nyear\nnursery\n3/2/22\n3/31/22\n2022\nPelargonium x hortorum\nCV-3\nMD\n\n\n22-302\ntest/data/reads/22-302_R1.fastq.gz\ntest/data/reads/22-302_R2.fastq.gz\nIllumina\n\n\nxan_test\nsubgroup\nyear\nnursery\n3/2/22\n4/1/22\n2022\nPelargonium x hortorum\nCV-4\nMD\n\n\n22-303\ntest/data/reads/22-303_R1.fastq.gz\ntest/data/reads/22-303_R2.fastq.gz\nIllumina\n\n\nxan_test\nsubgroup\nyear\nnursery\n3/2/22\n4/2/22\n2022\nPelargonium x hortorum\nCV-5\nMD\n\n\n22-304\ntest/data/reads/22-304_R1.fastq.gz\ntest/data/reads/22-304_R2.fastq.gz\nIllumina\n\n\nxan_test\nsubgroup\nyear\nnursery\n3/7/22\n4/3/22\n2022\nPelargonium x hortorum\nCV-6\nMD\n\n\n\n\n\n\nThere is quite a bit of information in this file, but only a few columns are essential (and can be in any order). The input csv needs show the pipeline where to find the sequencing reads. These can either be present locally or they can be downloaded from the NCBI.\nSample ID: The “sample_id” column is used to name your samples. This information will be used in graphs, so it is recommended to keep names short but informative. If you do not include this column, sample IDs will be generated from the names of your fastq files.\nUsing local reads: Columns “path_1” and “path_2” specify the path to forward and reverse reads. Each row corresponds to one individual sample. Reads for this tutorial are hosted on the pathogensurveilance github repo. . If your reads are single-ended, “path_2” should be left blank.\nShortread/Longread sequences*: Information in the column “sequencing_type” tells the pipeline these are derived from illumina shortreads. Other options for this column are “nanopore” and “pacbio”.\nDownloading reads: Sequence files may instead be hosted on the NCBI. In that case, the “shortread_1/shortread_2” columns should be substituted with a single “SRA” column, and they will be downloaded right after the pipeline checks the sample sheet. These downloads will show up in the folder path_surveil_data/reads. See test/data/metadata/xanthomonas.csv for an example using this input format.\nSpecifying a reference genome (optional): The “reference_refseq” column may be useful when you are relatively confident as to the identity of your samples and would like to include one particular reference for comparison. See Example 2 for an explanation of how to designate mandatory and optional references.\nAssigning sample groups (optional): The optional column “color_by” is used for data visualization. It will assign one or more columns to serve as grouping factors for the output report. Here, samples will be grouped by the values of the “year” and “nursery” columns. Note that multiple factors need to be separated by semicolons within the color_by column. \n\n\nRunning the pipeline\nHere is the full command used execute this example, using a docker container:\nnextflow run nf-core/pathogensurveillance --inout https://raw.githubusercontent.com/grunwaldlab/pathogensurveillance/master/test/data/metadata/xanthomonas.csv --outdir xanthomonas --download_bakta_db true -profile docker -resume --max_cpus 8 --max_memory 30GB -resume\nWhen running your own analysis, you will need to provide your own path to the input CSV file.\nBy default, the pipeline will run on 128 GB of RAM and 16 threads. These are more resources than are strictly necessary and beyond the capacity of most desktop computers. We can scale this back a bit for this lightweight test run. This analysis will work with 8 CPUs and 30 GB of RAM (albeit more slowly), which is specified by the –max_cpus and –max_memory settings.\nThe setting -resume is only necessary when resuming a previous analysis. However, it doesn’t hurt to include it at the start. If the pipeline is interrupted, this setting allows progress to pick up where it left off – as long as the previous command is executed from the same working directory.\nIf the pipeline begins successfully, you should see a screen tracking your progress:\n[25/63dcee] process &gt; PATHOGENSURVEILLANCE:INPUT_CHECK:SAMPLESHEET_CHECK (xanthomonas.csv)[100%] 1 of 1\n[-        ] process &gt; PATHOGENSURVEILLANCE:SRATOOLS_FASTERQDUMP                              -\n[-        ] process &gt; PATHOGENSURVEILLANCE:DOWNLOAD_ASSEMBLIES                               -\n[-        ] process &gt; PATHOGENSURVEILLANCE:SEQKIT_SLIDING                                    -\n[-        ] process &gt; PATHOGENSURVEILLANCE:FASTQC                                            -\n[-        ] process &gt; PATHOGENSURVEILLANCE:COARSE_SAMPLE_TAXONOMY:BBMAP_SENDSKETCH           -\nThe input and output of each process can be accessed from the work/ directory. The subdirectory within work/ is designated by the string to left of each step. Note that this location will be different each time the pipeline is run, and only the first part of the name of the subdirectory is shown. For this run, we could navigate to work/25/63dcee(etc) to access the input csv that is used for the next step. \n\n\nReport\nYou should see a message similar to this if the pipeline finishes successfully:\n-[nf-core/plantpathsurveil] Pipeline completed successfully-\n\nTo clean the cache, enter the command: \nnextflow clean evil_boyd -f \n\nCompleted at: 20-May-2024 12:44:40\nDuration    : 3h 29m 2s\nCPU hours   : 15.2\nSucceeded   : 253\nThe final report can be viewed as either a .pdf or .html file. It can be accessed inside the reports folder of the output directory (here: xanthomonas/reports). This report shows several key pieces of information about your samples.\nA note on storage management - pathogensurveillance creates a large number of intermediate files. For most users we recommend clearing these files after each run. To do so, run the script shown after the completion message (nextflow clean  -f). You would not want to do this if: (1) You still need to use the caching system. For example, imagine you would like to compare a new sample to 10 samples from a previous run. In that case, some files could be reused to make the pipeline work more quickly. (2) You would like to use intermediate files for your own analysis. By default, these files are saved in the output directory as symlinks to their location in the work/ directory, so you would need to retrieve these before clearing the cache. You could use alternatively use the option –copymode high to save all intermediate files to the published directory, though in the short term this doubles the storage footprint of each run.\nThis particular report has been included as an example \n\nSummary:\n\nPipeline Status Report: error messages for samples or sample groups\nInput Data: Data read from the input .csv file\n\n\nIdentification:\n\nInitial identification: Coarse identification from the bbmap sendsketch step. The first tab shows best species ID for each sample. The second tab shows similarity metrics between sample sequences and other reference genomes: %ANI (average nucleotide identity), %WKID (weighted kmer identity), and %completeness.\n\nFor more information about each metric, click the About this table tab underneath.\n\n\n\n\nMost similar organisms: Shows relationships between samples and references using % ani and % pocp (percentage of conserved proteins). For better resolution, you can interactively zoom in/out of plots.\n\n\nCore gene phylogeny: A core gene phylogeny uses the sequences of all gene shared by all of the genomes included in the tree to infer evolutionary relationships. It is the most robust identification provided by this pipeline, but its precision is still limited by the availability of similar reference sequences. Methods to generate this tree differ between prokaryotes and eukaryotes. Our input to the pipeline was prokaryotic DNA sequences, and the method to build this tree is based upon many different core genes shared between samples and references (for eukaryotes, this is constrained to BUSCO genes). This tree is built with iqtree and based upon shared core genes analyzed using the program pirate.\n\n\n\nSNP trees: This tree is better suited for visualizing the genetic diversity among samples. However, the core gene phylogeny provides a much better source of information for evolutionary differences among samples and other known references.\n\n\nMinimum spanning network\n\nMinimum spanning network: The nodes represent unique multilocus genotypes, and the size of nodes is proportional to the # number of samples that share the same genotype. The edges represent the SNP differences between two given genotypes, and the darker the color of the edges, the fewer SNP differences between the two.",
    "crumbs": [
      "Tutorial"
    ]
  },
  {
    "objectID": "tutorial.html#example-2-defining-references",
    "href": "tutorial.html#example-2-defining-references",
    "title": "Tutorial",
    "section": "Example 2: Defining References",
    "text": "Example 2: Defining References\nIf you know what your samples are already, you may want to tell the pipeline to use a “standard” reference genome instead of picking one that is more obscure – even if pathogensurveillance deems the alternative to be a better fit. Other users may have a few different organisms of interest that they want to use as a points of comparison. For example, maybe there is a particularly nasty strain of V. cholerae that you want to see in relation to your other samples. There are a few options to select (or not select) reference genomes for these cases.\nPathogensurveillance uses two categories of reference genomes. Primary references are used for alignment and will always be displayed in phylogenetic trees. In contrast, contextual references are selected before the primary reference is known and they may or may not be used later. Some contextual references are chosen because they are really close matches to your samples, and these may be selected to become primary references. However, pathogensurveillance will select a few distantly related contextual references too. Some of these are used to “fill out” the phylogeny, and you may want a higher or lower number of contextual references depending on how you want your phylogenetic trees to look.\n\nChosing primary references\nTake this sample list containing three Mycobacterium abscessus samples and three Mycobacterium leprae samples:\n\n\n\n\n\n\nsample_id\nncbi.accession\n\n\n\n\nmycobacterium_abscessus1\nERR7253671\n\n\nmycobacterium_abscessus2\nERR7253669\n\n\nmycobacterium_abscessus3\nERR7253671\n\n\nmycobacterium_leperae1\nSRR6241707\n\n\nmycobacterium_leperae2\nSRR6241708\n\n\nmycobacterium_leperae3\nSRR6241709\n\n\n\n\n\n\nTo force the pipeline to use the NCBI specified Mycobacterium abscessus reference genome for the three Mycobacterium abscessus samples, and likewise make the three Mycobacterium leprae samples use the NCBI specified Mycobacterium leprae genome, we need to tell pathogensurveillance both where to find these reference sequences and how to use them. We can either specify a local path to the reads, or this can instead be specified through the ref_ncbi_accession column. Here, how the references are used here is controlled by the ref_primary_usage column:\n\n\n\n\n\n\n\n\n\n\n\n\nsample_id\nncbi.accession\nref_ncbi_accession\nref_primary_usage\n\n\n\n\nmycobacterium_abscessus1\nERR7253671\nGCF_001632805.1\nrequired\n\n\nmycobacterium_abscessus2\nERR7253669\nGCF_001632805.1\nrequired\n\n\nmycobacterium_abscessus3\nERR7253671\nGCF_001632805.1\nrequired\n\n\nmycobacterium_leprae1\nSRR6241707\nGCF_003253775.1\nrequired\n\n\nmycobacterium_leprae2\nSRR6241708\nGCF_003253775.1\nrequired\n\n\nmycobacterium_leprae3\nSRR6241709\nGCF_003253775.1\nrequired\n\n\n\n\n\n\n\n\n\nSpecifying contextual references\nTaking the previous Mycobacterium abscessus/leprae example, imagine we would like to see the comparison between Mycobacterium abscessus and Mycobacterium tuberculosis. We can do this by including Mycobacterium tuberculosis as a mandatory contextual reference:\n\n\n\n\n\n\n\n\n\n\n\n\nsample_id\nncbi.accession\nref_ncbi_accession\nref_contextual_usage\n\n\n\n\nmycobacterium_abscessus1\nERR7253671\nGCF_001632805.1\nrequired\n\n\nmycobacterium_abscessus2\nERR7253669\nGCF_001632805.1\nrequired\n\n\nmycobacterium_abscessus3\nERR7253671\nGCF_001632805.1\nrequired\n\n\nmycobacterium_leprae1\nSRR6241707\n\n\n\n\nmycobacterium_leprae2\nSRR6241708\n\n\n\n\nmycobacterium_leprae3\nSRR6241709\n\n\n\n\n\n\n\n\n\n\n\nSelecting references from an NCBI query\nIt is also possible to submit a valid NCBI query to the pipeline with reference genomes selected from query hits. For example, you could test how your Mycobacterium leprae samples compared to a bunch of different other Mycobacterium leprae genomes:\n\n\n\n\n\n\n\n\n\n\n\n\nsample_id\nncbi.accession\nref_ncbi_query\nref_ncbi_query_max\n\n\n\n\nmycobacterium_abscessus1\nERR7253671\n\nNA\n\n\nmycobacterium_abscessus2\nERR7253669\n\nNA\n\n\nmycobacterium_abscessus3\nERR7253671\n\nNA\n\n\nmycobacterium_leprae1\nSRR6241707\nmycobacterium leprae\n100\n\n\nmycobacterium_leprae2\nSRR6241708\nmycobacterium leprae\n100\n\n\nmycobacterium_leprae3\nSRR6241709\nmycobacterium leprae\n100\n\n\n\n\n\n\nSome things to keep in mind:\n\nDepending on your organism, this may a massive amount of data. Make sure you have queried NCBI beforehand to get a good handle on how many references you are downloading.\nThe optional parameter ref_ncbi_query_max is a good way of limiting this number when you are sampling from a densely populated clade, such as Mycobacterium leprae. This parameter can either be a set number (like shown here) or a percentage.\nThe NCBI API will fail if there are too many requests. See ncbi support for more detail.\n\n\n\n\nMultiple references per sample\nIf we would like to add multiple references per sample, we can enter this information through a separate reference csv. In this example, we specify one primary reference each for Mycobacterium abscessus and Mycobacterium leprae, then three additional contextual references for Mycobacterium leprae:\n\n\n\n\n\n\n\n\n\n\n\n\nref_group_ids\nref_path\nRef.primary.usage\nRef.contextual.Usage\n\n\n\n\nabscessus\ntest/data/refs/mycobacterium_abscessus_reference1.fna\nrequired\n\n\n\nleprae\ntest/data/refs/mycobacterium_leprae_reference1.fna\nrequired\n\n\n\nleprae\ntest/data/refs/mycobacterium_leprae_reference2.fna\n\noptional\n\n\nleprae\ntest/data/refs/mycobacterium_leprae_reference3.fna\n\noptional\n\n\nleprae\ntest/data/refs/mycobacterium_leprae_reference4.fna\n\noptional\n\n\n\n\n\n\nNote that the “ref_group_ids” column in the sample input csv needs to match the sample csv:\n\n\n\n\n\n\nsample_id\nncbi.accession\nref_group_ids\n\n\n\n\nmycobacterium_abscessus1\nERR7253671\nabscessus\n\n\nmycobacterium_abscessus2\nERR7253669\nabscessus\n\n\nmycobacterium_abscessus3\nERR7253671\nabscessus\n\n\nmycobacterium_leprae1\nSRR6241707\nleprae\n\n\nmycobacterium_leprae2\nSRR6241708\nleprae\n\n\nmycobacterium_leprae3\nSRR6241709\nleprae\n\n\n\n\n\n\nThe path to this reference csv needs to be specified in the command to run the pipeline:\nnextflow run nf-core/pathogensurveillance --sample_data mycobacterium_samples.csv --reference_input mycobacterium_references.csv --out_dir mycobacterium_test --download_bakta_db true -profile docker",
    "crumbs": [
      "Tutorial"
    ]
  },
  {
    "objectID": "index.html#quickstart",
    "href": "index.html#quickstart",
    "title": "",
    "section": "Quickstart",
    "text": "Quickstart\n\nInstall Nextflow (&gt;=21.10.3)\nInstall any of Docker, Singularity (you can follow this tutorial), Podman, Shifter or Charliecloud for full pipeline reproducibility (you can use Conda both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see docs).\nDownload the pipeline and test it on a minimal dataset with a single command:\nnextflow run nf-core/pathogensurveillance -profile test,YOURPROFILE --outdir &lt;OUTDIR&gt; -resume \nStart running your own analysis:\n\nnextflow run nf-core/pathogensurveillance --input &lt;samplesheet.csv&gt; --outdir &lt;OUTDIR&gt;  -profile &lt;docker/singularity/podman/shifter/charliecloud/conda/institute&gt; -resume\n\nYou can also try running a small example dataset hosted with the source code using the following command (no need to download anything):\nnextflow run nf-core/pathogensurveillance --input https://raw.githubusercontent.com/grunwaldlab/pathogensurveillance/master/test/data/metadata_small.csv --outdir test_out -profile docker -resume",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "",
    "section": "Usage",
    "text": "Usage\n\n[!NOTE] If you are new to Nextflow and nf-core, please refer to this page on how to set-up Nextflow. Make sure to test your setup with -profile test before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\nnextflow run nf-core/pathogensurveillance -r dev -profile RUN_TOOL,xanthomonas_small -resume --out_dir test_output\nNote that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (RUN_TOOL in the example command above). You can chain multiple config profiles in a comma-separated string.\n\n\nThe pipeline comes with config profiles called docker, singularity, podman, shifter, charliecloud and conda which instruct the pipeline to use the named tool for software management. For example, -profile xanthomonas_small,docker.\nPlease check nf-core/configs to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use -profile &lt;institute&gt; in your command. This will enable either docker or singularity and set the appropriate execution settings for your local compute environment.\nIf you are using singularity, please use the nf-core download command to download images first, before running the pipeline. Setting the NXF_SINGULARITY_CACHEDIR or singularity.cacheDir Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\nIf you are using conda, it is highly recommended to use the NXF_CONDA_CACHEDIR or conda.cacheDir settings to store the environments in a central location for future pipeline runs.\n\n\n–&gt;\nNow, you can run the pipeline using:\nnextflow run nf-core/pathogensurveillance -r dev -profile RUN_TOOL -resume --sample_data &lt;TSV/CSV&gt; --out_dir &lt;OUTDIR&gt; --download_bakta_db\nnextflow run nf-core/pathogensurveillance \\\n   -profile &lt;docker/singularity/.../institute&gt; \\\n   --input samplesheet.tsv \\\n   --outdir &lt;OUTDIR&gt;",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "publication/abstracts/2024_aps_pacific_corvallis.html",
    "href": "publication/abstracts/2024_aps_pacific_corvallis.html",
    "title": "Pathogensurveillance: An automated computational pipeline for identification, population genomics, and monitoring of pathogens",
    "section": "",
    "text": "Pathogensurveillance: An automated computational pipeline for identification, population genomics, and monitoring of pathogens\nZSL Foster1, M Sudermann2, C Parada-Rojas2, F Iruegas-Bocardo2, R Alcala-Briseno2, JH Chang2, and NJ Grünwald1\n1 Horticultural Crops Disease and Pest Management Research Unit, USDA ARS, Corvallis, Oregon, USA\n2 Dept. Botany and Plant Pathology, Oregon State University, Corvallis, Oregon, USA\nRapid and automated analysis of plant pathogen genome sequences is essential for more effective responses to disease outbreaks. To account for the computational complexities of whole genome sequence analysis, we developed the pathogensurveillance pipeline to identify individuals or groups of pathogens and their diversity. Pathogensurveillance analyzes raw reads derived from eukaryotic or prokaryotic pathogens. The pipeline automatically determines the taxonomic placement, finds the closest reference genome sequence, maps sequence reads, calls variants, identifies core genes, and reports back phylogenetic relationships and identifications. References are selected automatically from the NCBI RefSeq database or can be supplied by the user. For each user-defined group of samples, pathogensurveillance produces an HTML report with summary statistics, interactive figures, and a static PDF report. Additionally, users can access the intermediate files in order to conduct further analyses. This pipeline can be executed on any Linux computer or cloud computing environment. Our pipeline automates and accelerates analysis of whole genome sequence data, which is essential for rapid responses to disease outbreaks, and also allows genomic analysis by users with less experience in biocomputing."
  },
  {
    "objectID": "publication/output.html",
    "href": "publication/output.html",
    "title": "nf-core/pathogensurveillance: Output",
    "section": "",
    "text": "This document describes the output produced by the pipeline. Most of the plots are taken from the MultiQC report, which summarises results at the end of the pipeline.\nThe directories listed below will be created in the results directory after the pipeline has finished. All paths are relative to the top-level results directory.\n\n\n\n\nThe pipeline is built using Nextflow and processes data using the following steps:\n\nFastQC - Raw read QC\nMultiQC - Aggregate report describing results and QC from the whole pipeline\nPipeline information - Report metrics generated during the workflow execution\n\n\n\n\n\nOutput files\n\n\nfastqc/\n\n*_fastqc.html: FastQC report containing quality metrics.\n*_fastqc.zip: Zip archive containing the FastQC report, tab-delimited data file and plot images.\n\n\n\nFastQC gives general quality metrics about your sequenced reads. It provides information about the quality score distribution across your reads, per base sequence content (%A/T/G/C), adapter contamination and overrepresented sequences. For further reading and documentation see the FastQC help pages.\n\n\n\n\n\nOutput files\n\n\nmultiqc/\n\nmultiqc_report.html: a standalone HTML file that can be viewed in your web browser.\nmultiqc_data/: directory containing parsed statistics from the different tools used in the pipeline.\nmultiqc_plots/: directory containing static images from the report in various formats.\n\n\n\nMultiQC is a visualization tool that generates a single HTML report summarising all samples in your project. Most of the pipeline QC results are visualised in the report and further statistics are available in the report data directory.\nResults generated by MultiQC collate pipeline QC from supported tools e.g. FastQC. The pipeline has special steps which also allow the software versions to be reported in the MultiQC output for future traceability. For more information about how to use MultiQC reports, see http://multiqc.info.\n\n\n\n\n\nOutput files\n\n\npipeline_info/\n\nReports generated by Nextflow: execution_report.html, execution_timeline.html, execution_trace.txt and pipeline_dag.dot/pipeline_dag.svg.\nReports generated by the pipeline: pipeline_report.html, pipeline_report.txt and software_versions.yml. The pipeline_report* files will only be present if the --email / --email_on_fail parameter’s are used when running the pipeline.\nReformatted samplesheet files used as input to the pipeline: samplesheet.valid.csv.\nParameters used by the pipeline run: params.json.\n\n\n\nNextflow provides excellent functionality for generating various reports relevant to the running and execution of the pipeline. This will allow you to troubleshoot errors with the running of the pipeline, and also provide you with other information such as launch commands, run times and resource usage."
  },
  {
    "objectID": "publication/output.html#introduction",
    "href": "publication/output.html#introduction",
    "title": "nf-core/pathogensurveillance: Output",
    "section": "",
    "text": "This document describes the output produced by the pipeline. Most of the plots are taken from the MultiQC report, which summarises results at the end of the pipeline.\nThe directories listed below will be created in the results directory after the pipeline has finished. All paths are relative to the top-level results directory."
  },
  {
    "objectID": "publication/output.html#pipeline-overview",
    "href": "publication/output.html#pipeline-overview",
    "title": "nf-core/pathogensurveillance: Output",
    "section": "",
    "text": "The pipeline is built using Nextflow and processes data using the following steps:\n\nFastQC - Raw read QC\nMultiQC - Aggregate report describing results and QC from the whole pipeline\nPipeline information - Report metrics generated during the workflow execution\n\n\n\n\n\nOutput files\n\n\nfastqc/\n\n*_fastqc.html: FastQC report containing quality metrics.\n*_fastqc.zip: Zip archive containing the FastQC report, tab-delimited data file and plot images.\n\n\n\nFastQC gives general quality metrics about your sequenced reads. It provides information about the quality score distribution across your reads, per base sequence content (%A/T/G/C), adapter contamination and overrepresented sequences. For further reading and documentation see the FastQC help pages.\n\n\n\n\n\nOutput files\n\n\nmultiqc/\n\nmultiqc_report.html: a standalone HTML file that can be viewed in your web browser.\nmultiqc_data/: directory containing parsed statistics from the different tools used in the pipeline.\nmultiqc_plots/: directory containing static images from the report in various formats.\n\n\n\nMultiQC is a visualization tool that generates a single HTML report summarising all samples in your project. Most of the pipeline QC results are visualised in the report and further statistics are available in the report data directory.\nResults generated by MultiQC collate pipeline QC from supported tools e.g. FastQC. The pipeline has special steps which also allow the software versions to be reported in the MultiQC output for future traceability. For more information about how to use MultiQC reports, see http://multiqc.info.\n\n\n\n\n\nOutput files\n\n\npipeline_info/\n\nReports generated by Nextflow: execution_report.html, execution_timeline.html, execution_trace.txt and pipeline_dag.dot/pipeline_dag.svg.\nReports generated by the pipeline: pipeline_report.html, pipeline_report.txt and software_versions.yml. The pipeline_report* files will only be present if the --email / --email_on_fail parameter’s are used when running the pipeline.\nReformatted samplesheet files used as input to the pipeline: samplesheet.valid.csv.\nParameters used by the pipeline run: params.json.\n\n\n\nNextflow provides excellent functionality for generating various reports relevant to the running and execution of the pipeline. This will allow you to troubleshoot errors with the running of the pipeline, and also provide you with other information such as launch commands, run times and resource usage."
  },
  {
    "objectID": "publication/usage.html",
    "href": "publication/usage.html",
    "title": "nf-core/pathogensurveillance: Usage",
    "section": "",
    "text": "Documentation of pipeline parameters is generated automatically from the pipeline schema and can no longer be found in markdown files.\n\n\n\n\n\n\n\n\nYou will need to create a samplesheet with information about the samples you would like to analyse before running the pipeline. Use this parameter to specify its location. It has to be a comma-separated file with 3 columns, and a header row as shown in the examples below.\n--input '[path to samplesheet file]'\n\n\nThe sample identifiers have to be the same when you have re-sequenced the same sample more than once e.g. to increase sequencing depth. The pipeline will concatenate the raw reads before performing any downstream analysis. Below is an example for the same sample sequenced across 3 lanes:\ncsv title=\"samplesheet.csv\" sample,fastq_1,fastq_2 CONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz CONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz CONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz\n\n\n\nThe pipeline will auto-detect whether a sample is single- or paired-end using the information provided in the samplesheet. The samplesheet can have as many columns as you desire, however, there is a strict requirement for the first 3 columns to match those defined in the table below.\nA final samplesheet file consisting of both single- and paired-end data may look something like the one below. This is for 6 samples, where TREATMENT_REP3 has been sequenced twice.\ncsv title=\"samplesheet.csv\" sample,fastq_1,fastq_2 CONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz CONTROL_REP2,AEG588A2_S2_L002_R1_001.fastq.gz,AEG588A2_S2_L002_R2_001.fastq.gz CONTROL_REP3,AEG588A3_S3_L002_R1_001.fastq.gz,AEG588A3_S3_L002_R2_001.fastq.gz TREATMENT_REP1,AEG588A4_S4_L003_R1_001.fastq.gz, TREATMENT_REP2,AEG588A5_S5_L003_R1_001.fastq.gz, TREATMENT_REP3,AEG588A6_S6_L003_R1_001.fastq.gz, TREATMENT_REP3,AEG588A6_S6_L004_R1_001.fastq.gz,\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nsample\nCustom sample name. This entry will be identical for multiple sequencing libraries/runs from the same sample. Spaces in sample names are automatically converted to underscores (_).\n\n\nfastq_1\nFull path to FastQ file for Illumina short reads 1. File has to be gzipped and have the extension “.fastq.gz” or “.fq.gz”.\n\n\nfastq_2\nFull path to FastQ file for Illumina short reads 2. File has to be gzipped and have the extension “.fastq.gz” or “.fq.gz”.\n\n\n\nAn example samplesheet has been provided with the pipeline.\n\n\n\n\nThe typical command for running the pipeline is as follows:\nnextflow run nf-core/pathogensurveillance --input ./samplesheet.csv --outdir ./results --genome GRCh37 -profile docker\nThis will launch the pipeline with the docker configuration profile. See below for more information about profiles.\nNote that the pipeline will create the following files in your working directory:\nwork                # Directory containing the nextflow working files\n&lt;OUTDIR&gt;            # Finished results in specified location (defined with --outdir)\n.nextflow_log       # Log file from Nextflow\n# Other nextflow hidden files, eg. history of pipeline runs and old logs.\nIf you wish to repeatedly use the same parameters for multiple runs, rather than specifying each flag in the command, you can specify these in a params file.\nPipeline settings can be provided in a yaml or json file via -params-file &lt;file&gt;.\n\n[!WARNING] Do not use -c &lt;file&gt; to specify parameters as this will result in errors. Custom config files specified with -c must only be used for tuning process resource specifications, other infrastructural tweaks (such as output directories), or module arguments (args).\n\nThe above pipeline run specified with a params file in yaml format:\nnextflow run nf-core/pathogensurveillance -profile docker -params-file params.yaml\nwith:\nyaml title=\"params.yaml\" input: './samplesheet.csv' outdir: './results/' genome: 'GRCh37' &lt;...&gt;\nYou can also generate such YAML/JSON files via nf-core/launch.\n\n\nWhen you run the above command, Nextflow automatically pulls the pipeline code from GitHub and stores it as a cached version. When running the pipeline after this, it will always use the cached version if available - even if the pipeline has been updated since. To make sure that you’re running the latest version of the pipeline, make sure that you regularly update the cached version of the pipeline:\nnextflow pull nf-core/pathogensurveillance\n\n\n\nIt is a good idea to specify the pipeline version when running the pipeline on your data. This ensures that a specific version of the pipeline code and software are used when you run your pipeline. If you keep using the same tag, you’ll be running the same version of the pipeline, even if there have been changes to the code since.\nFirst, go to the nf-core/pathogensurveillance releases page and find the latest pipeline version - numeric only (eg. 1.3.1). Then specify this when running the pipeline with -r (one hyphen) - eg. -r 1.3.1. Of course, you can switch to another version by changing the number after the -r flag.\nThis version number will be logged in reports when you run the pipeline, so that you’ll know what you used when you look back in the future. For example, at the bottom of the MultiQC reports.\nTo further assist in reproducibility, you can use share and reuse parameter files to repeat pipeline runs with the same settings without having to write out a command with every single parameter.\n\n[!TIP] If you wish to share such profile (such as upload as supplementary material for academic publications), make sure to NOT include cluster specific paths to files, nor institutional specific profiles.\n\n\n\n\n\n\n[!NOTE] These options are part of Nextflow and use a single hyphen (pipeline parameters use a double-hyphen)\n\n\n\nUse this parameter to choose a configuration profile. Profiles can give configuration presets for different compute environments.\nSeveral generic profiles are bundled with the pipeline which instruct the pipeline to use software packaged using different methods (Docker, Singularity, Podman, Shifter, Charliecloud, Apptainer, Conda) - see below.\n\n[!IMPORTANT] We highly recommend the use of Docker or Singularity containers for full pipeline reproducibility, however when this is not possible, Conda is also supported.\n\nThe pipeline also dynamically loads configurations from https://github.com/nf-core/configs when it runs, making multiple config profiles for various institutional clusters available at run time. For more information and to check if your system is supported, please see the nf-core/configs documentation.\nNote that multiple profiles can be loaded, for example: -profile test,docker - the order of arguments is important! They are loaded in sequence, so later profiles can overwrite earlier profiles.\nIf -profile is not specified, the pipeline will run locally and expect all software to be installed and available on the PATH. This is not recommended, since it can lead to different results on different machines dependent on the computer environment.\n\ntest\n\nA profile with a complete configuration for automated testing\nIncludes links to test data so needs no other parameters\n\ndocker\n\nA generic configuration profile to be used with Docker\n\nsingularity\n\nA generic configuration profile to be used with Singularity\n\npodman\n\nA generic configuration profile to be used with Podman\n\nshifter\n\nA generic configuration profile to be used with Shifter\n\ncharliecloud\n\nA generic configuration profile to be used with Charliecloud\n\napptainer\n\nA generic configuration profile to be used with Apptainer\n\nwave\n\nA generic configuration profile to enable Wave containers. Use together with one of the above (requires Nextflow 24.03.0-edge or later).\n\nconda\n\nA generic configuration profile to be used with Conda. Please only use Conda as a last resort i.e. when it’s not possible to run the pipeline with Docker, Singularity, Podman, Shifter, Charliecloud, or Apptainer.\n\n\n\n\n\nSpecify this when restarting a pipeline. Nextflow will use cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. For input to be considered the same, not only the names must be identical but the files’ contents as well. For more info about this parameter, see this blog post.\nYou can also supply a run name to resume a specific run: -resume [run-name]. Use the nextflow log command to show previous run names.\n\n\n\nSpecify the path to a specific config file (this is a core Nextflow command). See the nf-core website documentation for more information.\n\n\n\n\n\n\nWhilst the default requirements set within the pipeline will hopefully work for most people and with most input data, you may find that you want to customise the compute resources that the pipeline requests. Each step in the pipeline has a default set of requirements for number of CPUs, memory and time. For most of the pipeline steps, if the job exits with any of the error codes specified here it will automatically be resubmitted with higher resources request (2 x original, then 3 x original). If it still fails after the third attempt then the pipeline execution is stopped.\nTo change the resource requests, please see the max resources and tuning workflow resources section of the nf-core website.\n\n\n\nIn some cases, you may wish to change the container or conda environment used by a pipeline steps for a particular tool. By default, nf-core pipelines use containers and software from the biocontainers or bioconda projects. However, in some cases the pipeline specified version maybe out of date.\nTo use a different container from the default container or conda environment specified in a pipeline, please see the updating tool versions section of the nf-core website.\n\n\n\nA pipeline might not always support every possible argument or option of a particular tool used in pipeline. Fortunately, nf-core pipelines provide some freedom to users to insert additional parameters that the pipeline does not include by default.\nTo learn how to provide additional arguments to a particular tool of the pipeline, please see the customising tool arguments section of the nf-core website.\n\n\n\nIn most cases, you will only need to create a custom config as a one-off but if you and others within your organisation are likely to be running nf-core pipelines regularly and need to use the same settings regularly it may be a good idea to request that your custom config file is uploaded to the nf-core/configs git repository. Before you do this please can you test that the config file works with your pipeline of choice using the -c parameter. You can then create a pull request to the nf-core/configs repository with the addition of your config file, associated documentation file (see examples in nf-core/configs/docs), and amending nfcore_custom.config to include your custom profile.\nSee the main Nextflow documentation for more information about creating your own configuration files.\nIf you have any questions or issues please send us a message on Slack on the #configs channel.\n\n\n\n\nNextflow handles job submissions and supervises the running jobs. The Nextflow process must run until the pipeline is finished.\nThe Nextflow -bg flag launches Nextflow in the background, detached from your terminal so that the workflow does not stop if you log out of your session. The logs are saved to a file.\nAlternatively, you can use screen / tmux or similar tool to create a detached session which you can log back into at a later time. Some HPC setups also allow you to run nextflow within a cluster job submitted your job scheduler (from where it submits more jobs).\n\n\n\nIn some cases, the Nextflow Java virtual machines can start to request a large amount of memory. We recommend adding the following line to your environment to limit this (typically in ~/.bashrc or ~./bash_profile):\nNXF_OPTS='-Xms1g -Xmx4g'"
  },
  {
    "objectID": "publication/usage.html#warning-please-read-this-documentation-on-the-nf-core-website-httpsnf-co.repathogensurveillanceusage",
    "href": "publication/usage.html#warning-please-read-this-documentation-on-the-nf-core-website-httpsnf-co.repathogensurveillanceusage",
    "title": "nf-core/pathogensurveillance: Usage",
    "section": "",
    "text": "Documentation of pipeline parameters is generated automatically from the pipeline schema and can no longer be found in markdown files."
  },
  {
    "objectID": "publication/usage.html#samplesheet-input",
    "href": "publication/usage.html#samplesheet-input",
    "title": "nf-core/pathogensurveillance: Usage",
    "section": "",
    "text": "You will need to create a samplesheet with information about the samples you would like to analyse before running the pipeline. Use this parameter to specify its location. It has to be a comma-separated file with 3 columns, and a header row as shown in the examples below.\n--input '[path to samplesheet file]'\n\n\nThe sample identifiers have to be the same when you have re-sequenced the same sample more than once e.g. to increase sequencing depth. The pipeline will concatenate the raw reads before performing any downstream analysis. Below is an example for the same sample sequenced across 3 lanes:\ncsv title=\"samplesheet.csv\" sample,fastq_1,fastq_2 CONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz CONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz CONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz\n\n\n\nThe pipeline will auto-detect whether a sample is single- or paired-end using the information provided in the samplesheet. The samplesheet can have as many columns as you desire, however, there is a strict requirement for the first 3 columns to match those defined in the table below.\nA final samplesheet file consisting of both single- and paired-end data may look something like the one below. This is for 6 samples, where TREATMENT_REP3 has been sequenced twice.\ncsv title=\"samplesheet.csv\" sample,fastq_1,fastq_2 CONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz CONTROL_REP2,AEG588A2_S2_L002_R1_001.fastq.gz,AEG588A2_S2_L002_R2_001.fastq.gz CONTROL_REP3,AEG588A3_S3_L002_R1_001.fastq.gz,AEG588A3_S3_L002_R2_001.fastq.gz TREATMENT_REP1,AEG588A4_S4_L003_R1_001.fastq.gz, TREATMENT_REP2,AEG588A5_S5_L003_R1_001.fastq.gz, TREATMENT_REP3,AEG588A6_S6_L003_R1_001.fastq.gz, TREATMENT_REP3,AEG588A6_S6_L004_R1_001.fastq.gz,\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nsample\nCustom sample name. This entry will be identical for multiple sequencing libraries/runs from the same sample. Spaces in sample names are automatically converted to underscores (_).\n\n\nfastq_1\nFull path to FastQ file for Illumina short reads 1. File has to be gzipped and have the extension “.fastq.gz” or “.fq.gz”.\n\n\nfastq_2\nFull path to FastQ file for Illumina short reads 2. File has to be gzipped and have the extension “.fastq.gz” or “.fq.gz”.\n\n\n\nAn example samplesheet has been provided with the pipeline."
  },
  {
    "objectID": "publication/usage.html#running-the-pipeline",
    "href": "publication/usage.html#running-the-pipeline",
    "title": "nf-core/pathogensurveillance: Usage",
    "section": "",
    "text": "The typical command for running the pipeline is as follows:\nnextflow run nf-core/pathogensurveillance --input ./samplesheet.csv --outdir ./results --genome GRCh37 -profile docker\nThis will launch the pipeline with the docker configuration profile. See below for more information about profiles.\nNote that the pipeline will create the following files in your working directory:\nwork                # Directory containing the nextflow working files\n&lt;OUTDIR&gt;            # Finished results in specified location (defined with --outdir)\n.nextflow_log       # Log file from Nextflow\n# Other nextflow hidden files, eg. history of pipeline runs and old logs.\nIf you wish to repeatedly use the same parameters for multiple runs, rather than specifying each flag in the command, you can specify these in a params file.\nPipeline settings can be provided in a yaml or json file via -params-file &lt;file&gt;.\n\n[!WARNING] Do not use -c &lt;file&gt; to specify parameters as this will result in errors. Custom config files specified with -c must only be used for tuning process resource specifications, other infrastructural tweaks (such as output directories), or module arguments (args).\n\nThe above pipeline run specified with a params file in yaml format:\nnextflow run nf-core/pathogensurveillance -profile docker -params-file params.yaml\nwith:\nyaml title=\"params.yaml\" input: './samplesheet.csv' outdir: './results/' genome: 'GRCh37' &lt;...&gt;\nYou can also generate such YAML/JSON files via nf-core/launch.\n\n\nWhen you run the above command, Nextflow automatically pulls the pipeline code from GitHub and stores it as a cached version. When running the pipeline after this, it will always use the cached version if available - even if the pipeline has been updated since. To make sure that you’re running the latest version of the pipeline, make sure that you regularly update the cached version of the pipeline:\nnextflow pull nf-core/pathogensurveillance\n\n\n\nIt is a good idea to specify the pipeline version when running the pipeline on your data. This ensures that a specific version of the pipeline code and software are used when you run your pipeline. If you keep using the same tag, you’ll be running the same version of the pipeline, even if there have been changes to the code since.\nFirst, go to the nf-core/pathogensurveillance releases page and find the latest pipeline version - numeric only (eg. 1.3.1). Then specify this when running the pipeline with -r (one hyphen) - eg. -r 1.3.1. Of course, you can switch to another version by changing the number after the -r flag.\nThis version number will be logged in reports when you run the pipeline, so that you’ll know what you used when you look back in the future. For example, at the bottom of the MultiQC reports.\nTo further assist in reproducibility, you can use share and reuse parameter files to repeat pipeline runs with the same settings without having to write out a command with every single parameter.\n\n[!TIP] If you wish to share such profile (such as upload as supplementary material for academic publications), make sure to NOT include cluster specific paths to files, nor institutional specific profiles."
  },
  {
    "objectID": "publication/usage.html#core-nextflow-arguments",
    "href": "publication/usage.html#core-nextflow-arguments",
    "title": "nf-core/pathogensurveillance: Usage",
    "section": "",
    "text": "[!NOTE] These options are part of Nextflow and use a single hyphen (pipeline parameters use a double-hyphen)\n\n\n\nUse this parameter to choose a configuration profile. Profiles can give configuration presets for different compute environments.\nSeveral generic profiles are bundled with the pipeline which instruct the pipeline to use software packaged using different methods (Docker, Singularity, Podman, Shifter, Charliecloud, Apptainer, Conda) - see below.\n\n[!IMPORTANT] We highly recommend the use of Docker or Singularity containers for full pipeline reproducibility, however when this is not possible, Conda is also supported.\n\nThe pipeline also dynamically loads configurations from https://github.com/nf-core/configs when it runs, making multiple config profiles for various institutional clusters available at run time. For more information and to check if your system is supported, please see the nf-core/configs documentation.\nNote that multiple profiles can be loaded, for example: -profile test,docker - the order of arguments is important! They are loaded in sequence, so later profiles can overwrite earlier profiles.\nIf -profile is not specified, the pipeline will run locally and expect all software to be installed and available on the PATH. This is not recommended, since it can lead to different results on different machines dependent on the computer environment.\n\ntest\n\nA profile with a complete configuration for automated testing\nIncludes links to test data so needs no other parameters\n\ndocker\n\nA generic configuration profile to be used with Docker\n\nsingularity\n\nA generic configuration profile to be used with Singularity\n\npodman\n\nA generic configuration profile to be used with Podman\n\nshifter\n\nA generic configuration profile to be used with Shifter\n\ncharliecloud\n\nA generic configuration profile to be used with Charliecloud\n\napptainer\n\nA generic configuration profile to be used with Apptainer\n\nwave\n\nA generic configuration profile to enable Wave containers. Use together with one of the above (requires Nextflow 24.03.0-edge or later).\n\nconda\n\nA generic configuration profile to be used with Conda. Please only use Conda as a last resort i.e. when it’s not possible to run the pipeline with Docker, Singularity, Podman, Shifter, Charliecloud, or Apptainer.\n\n\n\n\n\nSpecify this when restarting a pipeline. Nextflow will use cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. For input to be considered the same, not only the names must be identical but the files’ contents as well. For more info about this parameter, see this blog post.\nYou can also supply a run name to resume a specific run: -resume [run-name]. Use the nextflow log command to show previous run names.\n\n\n\nSpecify the path to a specific config file (this is a core Nextflow command). See the nf-core website documentation for more information."
  },
  {
    "objectID": "publication/usage.html#custom-configuration",
    "href": "publication/usage.html#custom-configuration",
    "title": "nf-core/pathogensurveillance: Usage",
    "section": "",
    "text": "Whilst the default requirements set within the pipeline will hopefully work for most people and with most input data, you may find that you want to customise the compute resources that the pipeline requests. Each step in the pipeline has a default set of requirements for number of CPUs, memory and time. For most of the pipeline steps, if the job exits with any of the error codes specified here it will automatically be resubmitted with higher resources request (2 x original, then 3 x original). If it still fails after the third attempt then the pipeline execution is stopped.\nTo change the resource requests, please see the max resources and tuning workflow resources section of the nf-core website.\n\n\n\nIn some cases, you may wish to change the container or conda environment used by a pipeline steps for a particular tool. By default, nf-core pipelines use containers and software from the biocontainers or bioconda projects. However, in some cases the pipeline specified version maybe out of date.\nTo use a different container from the default container or conda environment specified in a pipeline, please see the updating tool versions section of the nf-core website.\n\n\n\nA pipeline might not always support every possible argument or option of a particular tool used in pipeline. Fortunately, nf-core pipelines provide some freedom to users to insert additional parameters that the pipeline does not include by default.\nTo learn how to provide additional arguments to a particular tool of the pipeline, please see the customising tool arguments section of the nf-core website.\n\n\n\nIn most cases, you will only need to create a custom config as a one-off but if you and others within your organisation are likely to be running nf-core pipelines regularly and need to use the same settings regularly it may be a good idea to request that your custom config file is uploaded to the nf-core/configs git repository. Before you do this please can you test that the config file works with your pipeline of choice using the -c parameter. You can then create a pull request to the nf-core/configs repository with the addition of your config file, associated documentation file (see examples in nf-core/configs/docs), and amending nfcore_custom.config to include your custom profile.\nSee the main Nextflow documentation for more information about creating your own configuration files.\nIf you have any questions or issues please send us a message on Slack on the #configs channel."
  },
  {
    "objectID": "publication/usage.html#running-in-the-background",
    "href": "publication/usage.html#running-in-the-background",
    "title": "nf-core/pathogensurveillance: Usage",
    "section": "",
    "text": "Nextflow handles job submissions and supervises the running jobs. The Nextflow process must run until the pipeline is finished.\nThe Nextflow -bg flag launches Nextflow in the background, detached from your terminal so that the workflow does not stop if you log out of your session. The logs are saved to a file.\nAlternatively, you can use screen / tmux or similar tool to create a detached session which you can log back into at a later time. Some HPC setups also allow you to run nextflow within a cluster job submitted your job scheduler (from where it submits more jobs)."
  },
  {
    "objectID": "publication/usage.html#nextflow-memory-requirements",
    "href": "publication/usage.html#nextflow-memory-requirements",
    "title": "nf-core/pathogensurveillance: Usage",
    "section": "",
    "text": "In some cases, the Nextflow Java virtual machines can start to request a large amount of memory. We recommend adding the following line to your environment to limit this (typically in ~/.bashrc or ~./bash_profile):\nNXF_OPTS='-Xms1g -Xmx4g'"
  },
  {
    "objectID": "publication/posters/poster_text.html",
    "href": "publication/posters/poster_text.html",
    "title": "",
    "section": "",
    "text": "Automated and rapid pipelines are needed to leverage the increasing availability of whole genome sequencing data. We are developing a pipeline for use in diagnostic clinics to automate the characterization of populations of pathogens. The pipeline is built with Nextflow, which provides a foundation for reproducible and scalable analyses. The pipeline accepts the paths to raw reads for one or more organisms and creates reports in the form of HTML websites or PDF documents. Significant features include the ability to analyze unidentified eukaryotic and prokaryotic samples, creation of reports for multiple user-defined groupings of samples, automated discovery and downloading of reference assemblies from NCBI RefSeq, and rapid initial identification based on k-mer sketches followed by a more robust core genome phylogeny."
  },
  {
    "objectID": "publication/posters/poster_text.html#summary",
    "href": "publication/posters/poster_text.html#summary",
    "title": "",
    "section": "",
    "text": "Automated and rapid pipelines are needed to leverage the increasing availability of whole genome sequencing data. We are developing a pipeline for use in diagnostic clinics to automate the characterization of populations of pathogens. The pipeline is built with Nextflow, which provides a foundation for reproducible and scalable analyses. The pipeline accepts the paths to raw reads for one or more organisms and creates reports in the form of HTML websites or PDF documents. Significant features include the ability to analyze unidentified eukaryotic and prokaryotic samples, creation of reports for multiple user-defined groupings of samples, automated discovery and downloading of reference assemblies from NCBI RefSeq, and rapid initial identification based on k-mer sketches followed by a more robust core genome phylogeny."
  },
  {
    "objectID": "publication/posters/poster_text.html#features-implemented",
    "href": "publication/posters/poster_text.html#features-implemented",
    "title": "",
    "section": "Features implemented",
    "text": "Features implemented\n\nGenome assembly and annotation (Prokaryotes only)\nCore genome phylogeny with RefSeq genomes for context (Prokaryotes only)\nVariant calling with a user-defined reference or one selected from RefSeq\nA minimum spanning network and SNP phylogeny from variant data\nReports for each user-defined group of samples as HTML websites with interactive figures or static PDF documents"
  },
  {
    "objectID": "publication/posters/poster_text.html#features-planned",
    "href": "publication/posters/poster_text.html#features-planned",
    "title": "",
    "section": "Features planned",
    "text": "Features planned\n\nAbility to use long reads such as those produced by Nanopore or PacBio or a mixture of short and long reads\nA phylogeny based on BUSCO genes extracted from raw reads with Read2Tree (Eukaryotes only)\nDetection of genes of interest, including antibiotic resistance loci, effectors, plasmids of interest, and any user-defined genes\nAn interactive map of sample locations annotated with information such as phylogenetic groupings or the presence of genes of interest\nAnalysis of samples from derived from tissues of an infected host\nAbility to identify viruses or other pathogenic organisms in environmental samples"
  },
  {
    "objectID": "publication/posters/poster_text.html#features-provided-by-nextflow",
    "href": "publication/posters/poster_text.html#features-provided-by-nextflow",
    "title": "",
    "section": "Features provided by Nextflow",
    "text": "Features provided by Nextflow\n\nAll programs needed to run the pipeline are installed automatically\nProcesses are run in parallel, allowing for analysis of massive data sets quickly if sufficient computing resources are available\nThe pipeline can be run on personal computers, high performance clusters, or commercial cloud services such as AWS\nInputs, outputs, and even the pipeline code itself can be stored anywhere on the internet and specified using URLs\nSamples can be added to an analysis without rerunning the entire pipeline.\nThe pipeline can pick up where it left off if it is interrupted\nEach process in the pipeline is run in its own docker/singularity container or conda environment, enabling reproducibility"
  },
  {
    "objectID": "publication/articles/draft.html",
    "href": "publication/articles/draft.html",
    "title": "Publication draft",
    "section": "",
    "text": "The pathogensurveillance pipeline automatically selects and download a set of references optimized for the analysis of a set of potentially diverse samples. These automatically selected references can be combined with or replaced by user-defined references, which can take the form of local files, URLs to files online, NCBI assembly accessions, or arbitrary queries to the NCBI assembly database. In order to make narrow down the taxa of references that need to be considered, the sample undergoes a tentative identification using bbmap sendsketch, a quick k-mer-hash-based comparison with an online database of similarly made hashes. Sendsketch returns the taxonomic classifications associated with the raw reads of the samples, including those of any potential contaminants or hosts. The NCBI assembly metadata is downloaded for every unique family found in all classifications for all samples. A subset of references present in this metadata is then selected on the basis of the genera, species, and strains in the taxonomic classifications returned by sendsketch for each sample as well as data about the references themselves, such as whether they are type strains and their overall quality/contiguity. The effects of potential inaccuracies in the initial sendsketch-based classification are mitigated by downloading representatives of each genus in each predicted family, each species in each predicted genus, and each strain in each predicted species. This means that even if the initial classification is so inaccurate that even the genus is incorrect, but the family is correct, then representatives of the correct genus will be selected and the user can then refine the reference selection or manually choose a reference. Selected references for each sample are downloaded and all references and samples are compared using sourmash sketch followed by sourmash compare. The resulting estimated average nucleotide identity (ANI) matrix is used to select mapping and contextual references for each subworkflow that requires them.\nContextual references are those used to provide taxonomic context in phylogenetic trees and other comparisons, in particular the core gene phylogeny for prokaryotes and the BUSCO gene phylogeny for eukaryotes. They are intended to represent a wide range of similarity to the samples so that samples can be understood in the context of the taxonomic family they belong to. Contextual references are chosen such that each sample has the reference it is most similar to as well as a selection of references equally spaced along a gradient of similarity. Similarity is determined using the ANI calculated by sourmash, as described above, and preference is given to type strains and those with normal binomial Latin names as opposed to numeric codes. Since diverse samples can be in the same group, one sample’s closest reference might function as another’s more distant reference. Conversely, a group of similar samples can likely use many of the same references. Therefore, all but the most similar references to each sample are chosen for the group as a whole in order to minimize the total number of references needed to provide context for every sample in the group. This process picks the minimum number of references needed for the group of samples which minimizes the computational resources needed to construct the phylogeny.\nMapping references are used to map reads for a group of samples to a single reference, as is done in the variant analysis subworkflow. Only samples mapped to the same reference can be compared, but the reference needs to be similar enough to every sample that reads can be aligned reliably. The ANI comparisons between samples and available references are used to progressively cluster samples with references based on how many samples a given reference is similar enough to, based on an adjustable threshold. If all samples in a group are similar enough, this process with result in a single cluster using the reference most similar to the greatest number of samples. When multiple clusters result, because the ANI difference between clusters is too great, variant analysis is conducted on each cluster separately. This process minimizes the number of references used and therefore maximizes comparisons between samples sharing the same reference."
  },
  {
    "objectID": "publication/articles/draft.html#reference-selection",
    "href": "publication/articles/draft.html#reference-selection",
    "title": "Publication draft",
    "section": "",
    "text": "The pathogensurveillance pipeline automatically selects and download a set of references optimized for the analysis of a set of potentially diverse samples. These automatically selected references can be combined with or replaced by user-defined references, which can take the form of local files, URLs to files online, NCBI assembly accessions, or arbitrary queries to the NCBI assembly database. In order to make narrow down the taxa of references that need to be considered, the sample undergoes a tentative identification using bbmap sendsketch, a quick k-mer-hash-based comparison with an online database of similarly made hashes. Sendsketch returns the taxonomic classifications associated with the raw reads of the samples, including those of any potential contaminants or hosts. The NCBI assembly metadata is downloaded for every unique family found in all classifications for all samples. A subset of references present in this metadata is then selected on the basis of the genera, species, and strains in the taxonomic classifications returned by sendsketch for each sample as well as data about the references themselves, such as whether they are type strains and their overall quality/contiguity. The effects of potential inaccuracies in the initial sendsketch-based classification are mitigated by downloading representatives of each genus in each predicted family, each species in each predicted genus, and each strain in each predicted species. This means that even if the initial classification is so inaccurate that even the genus is incorrect, but the family is correct, then representatives of the correct genus will be selected and the user can then refine the reference selection or manually choose a reference. Selected references for each sample are downloaded and all references and samples are compared using sourmash sketch followed by sourmash compare. The resulting estimated average nucleotide identity (ANI) matrix is used to select mapping and contextual references for each subworkflow that requires them.\nContextual references are those used to provide taxonomic context in phylogenetic trees and other comparisons, in particular the core gene phylogeny for prokaryotes and the BUSCO gene phylogeny for eukaryotes. They are intended to represent a wide range of similarity to the samples so that samples can be understood in the context of the taxonomic family they belong to. Contextual references are chosen such that each sample has the reference it is most similar to as well as a selection of references equally spaced along a gradient of similarity. Similarity is determined using the ANI calculated by sourmash, as described above, and preference is given to type strains and those with normal binomial Latin names as opposed to numeric codes. Since diverse samples can be in the same group, one sample’s closest reference might function as another’s more distant reference. Conversely, a group of similar samples can likely use many of the same references. Therefore, all but the most similar references to each sample are chosen for the group as a whole in order to minimize the total number of references needed to provide context for every sample in the group. This process picks the minimum number of references needed for the group of samples which minimizes the computational resources needed to construct the phylogeny.\nMapping references are used to map reads for a group of samples to a single reference, as is done in the variant analysis subworkflow. Only samples mapped to the same reference can be compared, but the reference needs to be similar enough to every sample that reads can be aligned reliably. The ANI comparisons between samples and available references are used to progressively cluster samples with references based on how many samples a given reference is similar enough to, based on an adjustable threshold. If all samples in a group are similar enough, this process with result in a single cluster using the reference most similar to the greatest number of samples. When multiple clusters result, because the ANI difference between clusters is too great, variant analysis is conducted on each cluster separately. This process minimizes the number of references used and therefore maximizes comparisons between samples sharing the same reference."
  },
  {
    "objectID": "publication/articles/draft.html#multigene-phylogeny",
    "href": "publication/articles/draft.html#multigene-phylogeny",
    "title": "Publication draft",
    "section": "Multigene phylogeny",
    "text": "Multigene phylogeny\nMultigene phylogenies for all samples and a selection of references are inferred from shared genes. This allows users to understand samples in the context of the overall tree of life, similar organisms available on public data bases, and user-defined references. In general, this is done by annotating genomes, identifying shared genes, and inferring a tree for each subset of samples and references that share enough genes. For prokaryotes, genome annotation is done using Bakta (https://doi.org/10.1099/mgen.0.000685) and shared genes are identified using PIRATE (https://doi.org/10.1093/gigascience/giz119), which uses pairwise comparisons of sequence data to identify orthologous genes. This approach identifies the maximum number of shared genes since it does not rely on the names or descriptions of annotated genes, but only on their location in the genome and sequence. However, such a computationally intensive approach would not be practical for large eukaryote genomes, so instead, single copy BUSCO (https://doi.org/10.1093/molbev/msab199) genes are identified in eukaryotic genomes and are considered orthologous if they share a name.\nSamples and references are then subset as needed into clusters that share enough genes to create a phylogeny. The minimum number of genes needed is 10 by default, but this threshold can be adjusted by the user. The subsets are determined using a greedy clustering algorithm, in which each sample/reference starts in its own cluster. Clusters that share the greatest number of genes are then combined until all samples/references are in the same cluster or further combinations would result in too few shared genes within a cluster. However, this last configuration of the clustering process is not necessarily the one chosen, since the addition of a single sample or reference could reduce the number of genes shared from 100s to little more than 10 and this might not be a desirable trade off, especially if it is the addition was a references rather than a sample or there are 100s of samples being analyzed. Instead, a record is kept of the state of each stage in the clustering process and a score is calculated for each potential clustering. This score incorporates, in order of decreasing importance, proportion of samples in clusters large enough to make trees, proportion of references in clusters large enough to make trees, the number of shared genes, and the mean size of clusters. The stage in the clustering process with the highest score is chosen. Finally, a multigene phylogeny is created using IQ-TREE (https://doi.org/10.1093/molbev/msaa015) for each cluster using all genes shared by that cluster. This clustering process ensures that useful outputs are created no matter how diverse the input sequences are."
  },
  {
    "objectID": "publication/articles/draft.html#variant-analysis",
    "href": "publication/articles/draft.html#variant-analysis",
    "title": "Publication draft",
    "section": "Variant analysis",
    "text": "Variant analysis"
  },
  {
    "objectID": "publication/abstracts/2024_aps_memphis.html",
    "href": "publication/abstracts/2024_aps_memphis.html",
    "title": "Pathogensurveillance: An automated computational pipeline for identification, population genomics, and monitoring of pathogens",
    "section": "",
    "text": "Pathogensurveillance: An automated computational pipeline for identification, population genomics, and monitoring of pathogens\nZSL Foster1, M Sudermann2, C Parada-Rojas2, F Iruegas-Bocardo2, R Alcala-Briseno2, JH Chang2, and NJ Grünwald1\n1 Horticultural Crops Disease and Pest Management Research Unit, USDA ARS, Corvallis, Oregon, USA\n2 Dept. Botany and Plant Pathology, Oregon State University, Corvallis, Oregon, USA\nRapid and automated analysis of plant pathogen genome sequences is needed for more effective responses to disease outbreaks, but its complexity necessitates advanced bioinformatic techniques. We developed the pathogensurveillance pipeline for the automated identification of individuals or groups of pathogens and exploration of their genomic diversity without the need for bioinformatics experience. Pathogensurveillance takes as input raw reads from potentially unidentified eukaryotic or prokaryotic organisms. The pipeline automatically determines a taxonomic placement, finds the closest reference genome sequences, maps sequence reads, calls variants, identifies core genes, and reports phylogenetic relationships and identifications. References are selected and downloaded automatically from the NCBI RefSeq database or can be supplied by the user. For each user-defined group of samples, an HTML report with statistics and interactive figures/tables is made. A static PDF version is also made. Advanced users can access the intermediate files to conduct further analyses. It can be executed on any Linux computer, from laptops to large-scale cloud computing environments. It is written in Nextflow, allowing it to take advantage of massive computational resources when available, restart failed analyses, or add samples to existing analyses without doing redundant work. Our pipeline automates and accelerates analysis of whole genome sequence data, which is essential for rapid responses to disease outbreaks, while allowing genomic analysis by non-bioinformaticians."
  },
  {
    "objectID": "documentation.html#documentation",
    "href": "documentation.html#documentation",
    "title": "Documentation",
    "section": "",
    "text": "For more details and further functionality, please refer to the usage documentation and the parameter documentation.\nTo see the results of an example test run with a full size dataset refer to the results tab on the nf-core website pipeline page. For more details about the output files and reports, please refer to the output documentation.\n\n\nThe primary input to the pipeline is a TSV (tab-separated value) or CSV (comma comma-separated value) file, specified using the --sample_data option. This can be made in a spreadsheet program like LibreOffice Calc or Microsoft Excel by exporting to TSV. Columns can be in any order and unneeded columns can be left out or left blank. Column names are case insensitive and spaces are equivalent to underscores and can be left out. Only a single column containing either paths to raw sequence data, SRA (Sequence Read Archive) accessions, or NCBI queries to search the SRA is required and each sample can have values in different columns. Any columns not recognized by pathogensurveillance will be ignored, allowing users to adapt existing sample metadata table by adding new columns. Below is a description of each column used by pathogensurveillance:\n\nsample_id: The unique identifier for each sample. This will be used in file names to distinguish samples in the output. Each sample ID must correspond to a single source of sequence data (e.g. the path and ncbi_accession columns), although the same sequence data can be used by different IDs. Any values supplied that correspond to different sources of sequence data or contain characters that cannot appear in file names (/:*?“&lt;&gt;| .) will be modified automatically. If not supplied, it will be inferred from the path, ncbi_accession, or name columns.\nname: A human-readable label for the sample that is used in plots and tables. If not supplied, it will be inferred from sample_id.\ndescription: A longer human-readable label that is used in plots and tables. If not supplied, it will be inferred from name.\npath: Path to input sequence data, typically gzipped FASTQ files. When paired end sequencing is used, this is used for the forward read’s data and path_2 is used for the reverse reads. This can be a local file path or a URL to an online location. The sequence_type column must have a value.\npath_2: Path to the FASTQ files for the reverse read when paired-end sequencing is used. This can be a local file path or a URL to an online location. The sequence_type column must have a value.\nncbi_accession: An SRA accession ID for reads to be downloaded and used as samples. Values in the sequence_type column will be looked up if not supplied.\nncbi_query: A valid NCBI search query to search the SRA for reads to download and use as samples. This will result in an unknown number of samples being analyzed. The total number downloaded is limited by the ncbi_query_max column. Values in the sample_id, name, and description columns will be append to that supplied by the user. Values in the sequence_type column will be looked up and does not need to be supplied by the user.\nncbi_query_max: The maximum number or percentage of samples downloaded for the corresponding query in the ncbi_query column. Adding a % to the end of a number indicates a percentage of the total number of results instead of a count. A random of subset of results will be downloaded if ncbi_query_max is less than “100%” or the total number of results.\nsequence_type: The type of sequencing used to produce reads for the reads_1 and reads_2 columns. Valid values include anything containing the words “illumina”, “nanopore”, or “pacbio”. Will be looked up automatically for ncbi_accession and ncbi_query inputs but must be supplied by the user for path inputs.\nreport_group_ids: How to group samples into reports. For every unique value in this column a report will be generated. Samples can be assigned to multiple reports by separating group IDs by “;”. For example all;subset will put the sample in both all and subset report groups. Samples will be added to a default group if this is not supplied.\ncolor_by: The names of other columns that contain values used to color samples in plots and figures in the report. Multiple column names can be separated by “;”. Specified columns can contain either categorical factors or specific colors, specified as a hex code. By default, samples will be one color and references another.\nploidy: The ploidy of the sample. Should be a number. Defaults to “1”.\nenabled: Either “TRUE” or “FALSE”, indicating whether the sample should be included in the analysis or not. Defaults to “TRUE”.\nref_group_ids: One or more reference group IDs separated by “;”. These are used to supply specific references to specific samples. These IDs correspond to IDs listed in the ref_group_ids or ref_id columns of the reference metadata TSV.\n\nAdditionally, users can supply a reference metadata TSV/CSV that can be used to assign custom references to particular samples using the --reference_data option. If not provided, the pipeline will download and choose references to use automatically. References are assigned to samples if they share a reference group ID in the ref_group_ids columns that can appear in both input TSVs/CSVs. The reference metadata TSV or the sample metadata TSV can have the following columns:\n\nref_group_ids: One or more reference group IDs separated by “;”. These are used to group references and supply an ID that can be used in the ref_group_ids column of the sample metadata TSV/CSV to assign references to particular samples.\nref_id: The unique identifier for each user-defined reference genome. This will be used in file names to distinguish samples in the output. Each reference ID must correspond to a single source of reference data (The ref_path, ref_ncbi_accession, and ref_ncbi_query columns), although the same reference data can be used by multiple IDs. Any values that correspond to different sources of reference data or contain characters that cannot appear in file names (/:*?“&lt;&gt;| .) will be modified automatically. If not supplied, it will be inferred from the path, ref_name columns or supplied automatically when ref_ncbi_accession or ref_ncbi_query are used.\nref_id: The unique identify for each reference input. This will be used in file names to distinguish references in the output. Each sample ID must correspond to a single source of reference data (e.g. the ref_path and ref_ncbi_accession columns), although the same sequence data can be used by different IDs. Any values supplied that correspond to different sources of reference data or contain characters that cannot appear in file names (/:*?“&lt;&gt;| .) will be modified automatically. If not supplied, it will be inferred from the ref_path, ref_ncbi_accession, or ref_name columns.\nref_name: A human-readable label for user-defined reference genomes that is used in plots and tables. If not supplied, it will be inferred from ref_id. It will be supplied automatically when the ref_ncbi_query column is used.\nref_description: A longer human-readable label for user-defined reference genomes that is used in plots and tables. If not supplied, it will be inferred from ref_name. It will be supplied automatically when the ref_ncbi_query column is used.\nref_path: Path to user-defined reference genomes for each sample. This can be a local file path or a URL to an online location.\nref_ncbi_accession: RefSeq accession ID for a user-defined reference genome. These will be automatically downloaded and used as input.\nref_ncbi_query: A valid NCBI search query to search the assembly database for genomes to download and use as references. This will result in an unknown number of references being downloaded. The total number downloaded is limited by the ref_ncbi_query_max column. Values in the ref_id, ref_name, and ref_description columns will be append to that supplied by the user.\nref_ncbi_query_max: The maximum number or percentage of references downloaded for the corresponding query in the ref_ncbi_query column. Adding a % to the end of a number indicates a percentage of the total number of results instead of a count. A random of subset of results will be downloaded if ncbi_query_max is less than “100%” or the total number of results.\nref_primary_usage: Controls how the reference is used in the analysis in cases where a single “best” reference is required, such as for variant calling. Can be one of “optional” (can be used if selected by the analysis), “required” (will always be used), “exclusive” (only those marked “exclusive” will be used), or “excluded” (will not be used).\nref_contextual_usage: Controls how the reference is used in the analysis in cases where multiple references are required to provide context for the samples, such as for phylogeny. Can be one of “optional” (can be used if selected by the analysis), “required” (will always be used), “exclusive” (only those marked “exclusive” will be used), or “excluded” (will not be used).\nref_color_by: The names of other columns that contain values used to color references in plots and figures in the report. Multiple column names can be separated by “;”. Specified columns can contain either categorical factors or specific colors, specified as a hex code. By default, samples will be one color and references another.\nref_enabled: Either “TRUE” or “FALSE”, indicating whether the reference should be included in the analysis or not. Defaults to “TRUE”.",
    "crumbs": [
      "Documentation"
    ]
  }
]